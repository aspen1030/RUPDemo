[["index.html", "RUPDemo Digitization Guide Preface", " RUPDemo Digitization Guide 2024-09-11 Preface This guide is an extension of the PADRINO Digitization Guide, aimed at expanding on the pre-processing and model fitting steps which precede the actual process of model digitization. Where the PADRINO Digitization Guide exists to support digitizers in prepping published IPMs to be added to the PADRINO database given that all information necessary to recreate the IPM is readily available, this guide supports digitizers in the more challenging task of parameterizing a new IPM from datasets which have not been modeled with an IPM before. Broadly, this guide serves as reference for digitizers in the RUPDemo project to become familiar with working with IPMs in R. The guide is divided into two sections. The first section, General Guidelines, consists of three chapters of reference material. The first chapter concerns coding best practices, the second chapter provides guidelines on project organization, and the third chapter is a step-by-step guide for the entire workflow. The second section, Adler Dataset Example, shows the workflow in action using a chart quadrat dataset published in Chu et al., 2013. This section is broken down into multiple chapters which consecutively add layers of complexity to the model. It is best to read the chapters in order. This is a living document which is currently still under construction. Chapter 7, an extension of the Bouteloua gracilis model which considers the effect of grazing treatment, is nearly completed and will be added to this document in the fall. A further extension which incorporates methods to conduct uncertainty analyses is planned and will be added soon. If you spot an error, please report the issue on GitHub. "],["codestd.html", "1 Coding Standards 1.1 Starting a new script 1.2 Organizing scripts 1.3 Formatting sections and comments 1.4 Coding style 1.5 Writing functions", " 1 Coding Standards This document defines standard for formatting and organizing scripts, as well as guidelines for coding style to promote readability and consistency in our scripts. Shared standards will facilitate clear communication across the workflow of the RUPDemo project. These standards are derived from the tidyverse style guide. 1.1 Starting a new script The first step of starting a new script is to give the file a short, descriptive name. The name should ideally begin with a number indicating the order in which the scripts have been written within the project followed by an underscore and a few words describing the main function of the script. More information about when to start a new script and how to organize a project can be found in Chapter 2 : Project Organization. Start the script by providing metadata about the script. Here is an example of the first 18 lines of a draft of this document: # Standards for writing code for RUPDemo # Aspen Workman # Summer 2024 # The following script provides guidelines and standards for writing code and # formatting scripts for the RUPDemo project using a demo dataset. rm( list = ls() ) options( stringsAsFactors = F ) library( tidyverse ) library( ggplot2 ) library( patchwork ) # Data ------------------------------------------------------------------------- demo_df &lt;- read.csv( &quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/demo.csv&quot; ) Let’s break down the sections which should be included before any content: Title: The title of the script communicates what the script aims to accomplish, or what makes it different from similar scripts. Author: Who wrote the script? Include your full name and the name(s) of anyone else who contributed sections of the code. If you are pasting in code from someone else’s script in the group, include their name. Date: About when were you working on this script? Give a general date. Short description of the script: If your title is informative enough, then you may not need any other description. However, sometimes it is useful to provide more details about the script in order to properly differentiate it from other scripts, i.e. which version of the model are you working on? Which dataset does this script rely on, and what makes it different? What assumptions are you making about the model? Does this script only focus on a single species from a multi-species publication? This is also the section where you would include the DOI and other citation information in your data formatting script. Libraries: Clean up the environment if needed, configure settings, and list the packages you will use. Do not load packages in the middle of the script; if you find yourself needing a different package, make sure to scroll back to the top of your script and add it to the list! Data: Under a new section header, import the data that you will use in the script. As with packages, do not import new data in the middle of the script; add them to the Data section in the beginning even if you don’t need them until later! After these beginning sections, add a new section header to describe the first step of your script and begin writing! 1.2 Organizing scripts Use section headers (example below) to initialize a new section of your script. These headers can help you navigate through your script by opening the document outline (top right icon in Source pane). # Fitting linear models -------------------------------------------------------- Avoid nesting of headers. The headers should describe the overall goal of a section of code, and they should not be a line-by-line commentary. Avoid using subheaders to describe each step; simply use a comment line to describe the next step. Dividing your script into sections can be somewhat arbitrary. The main goal is to maintain continuity within a project. Be consistent with other scripts you’ve written, and aim to have fewer sections rather than too many. 1.3 Formatting sections and comments The main things to keep in mind when writing a new script are readability and consistency. Try to keep all text within the document bounds (line on right side). Use spaces and blank lines to break things up and be consistent throughout the entire script. Section headers should be followed by a blank line before any comments or code begin. End a section with two blank lines before the next section header. Use single blank lines throughout a section to separate different chunks of code. Comments should always come before the chunk of code they refer to. Begin a comment and include a space after the # before writing any text. Do not put a blank line between a comment and the code it describes. Not all code needs comments, but it is nice to keep track of what you are doing. Comments extending onto a second line should be indented with a tab. Comments can also be written inside of multi-line piping or ggplots to explain different parts of the code as necessary. Embedded comments should be aligned with the code and presented on their own line(s) before the line the comment explains. Ex.: # Data formatting -------------------------------------------------------------- # Subset the data for 2008 and 2009, group the data by species, and calculate # the mean change in size for each species mean_growth_2008 &lt;- demo_df %&gt;% # select only data from the year 2008 subset( year == 2008 ) %&gt;% # group the data by species group_by( species ) %&gt;% # calculate the mean change in size for each species summarize( mean_growth = mean( delta_size ) ) mean_growth_2009 &lt;- demo_df %&gt;% # no need to add comments here because it&#39;s the same code subset( year == 2009 ) %&gt;% group_by( species ) %&gt;% summarize( mean_growth = mean( delta_size ) ) 1.4 Coding style Again, please prioritize readability and maintain consistency as you code. Everyone has their own style of coding, and these guidelines should serve as a starting point to keep everything clear and consistent. Use spaces between all separate objects and operators, including as a buffer around objects within parentheses. Ex.: # General example: function( object ) * function( object ) growth_a_2010 &lt;- demo_df %&gt;% subset( year == 2010 &amp; species == &quot;A&quot; ) %&gt;% mutate( percent_change = ( delta_size / size_t0 ) * 100 ) In longer chunks of code, align all &lt;- and be sure to put spaces before and after the &lt;-. In general, keep all code aligned in easy-to-read columns and don’t be afraid to break up function inputs across multiple lines. Again, try to keep everything within the document bounds. Ex.: growth_a &lt;- demo_df %&gt;% subset( species == &quot;A&quot; ) %&gt;% mutate( percent_change = ( delta_size / size_t0 ) * 100, log_size_t0 = log( size_t0 ) ) growth_b_2012 &lt;- demo_df %&gt;% subset( species == &quot;B&quot; &amp; year == 2012 ) %&gt;% mutate( percent_change = ( delta_size / size_t0 ) * 100, log_size_t0 = log( size_t0 ) ) Give your objects and functions short but descriptive names. Do not use capital letters. Use underscores (_) as separators in object names; do not use periods, as they can cause issues with certain functions. Use “_df” and “_list” or other easy to understand conventions as suffixes to name similar objects of different types. Be descriptive whenever possible and avoid using general names. Numbers should only be used when they are meaningful, i.e. to indicate the year in the previous examples. Never use names like “df3” unless you are creating a temporary object to test out a function you are working on and excluding the temporary object from the script will not impact the functionality of the script. 1.5 Writing functions You should organize the code using functions whenever possible. As a rule of thumb, when the same operation is repeated twice, then use a function to perform it! This will save time when you notice an error in the input or want to test out different subsets of the data, for example. Instead of writing a for loop, write a function( i ) and then use lapply( 1:n, function ) to run the function n times. Ex.: mean_growth_species &lt;- function( i ) { sp_temp &lt;- c( &quot;A&quot;, &quot;B&quot; )[i] df_temp &lt;- demo_df %&gt;% subset( species == sp_temp ) %&gt;% group_by( year ) %&gt;% summarize( mean_growth = mean( delta_size ) ) return( df_temp ) } mean_growth_list &lt;- lapply( 1:2, mean_growth_species ) names(mean_growth_list) &lt;- c( &quot;A&quot;, &quot;B&quot; ) mean_growth_df &lt;- bind_rows( mean_growth_list, .id = &quot;id&quot; ) Do whatever you can to avoid writing nested for loops. Many times, ecological data is nested - in our case, multiple sites within years. In most, if not all of these cases, you can avoid writing nested loops using the function expand.grid( ) to create a dataframe including every combination of two variables, and then use lapply( ) to apply a function across all levels of combinations instead of writing a nested for loop. Ex. (only for the years 2005-2008): design_df &lt;- expand.grid( sp_i = c( &quot;A&quot;, &quot;B&quot; ), yr_i = 2005:2008 ) plot_growth &lt;- function( i ) { temp_df &lt;- demo_df %&gt;% subset( species == design_df$sp_i[i] &amp; year == design_df$yr_i[i] ) temp_mod &lt;- lm( size_t1 ~ size_t0, data = temp_df ) pred_df &lt;- data.frame( size_t1_pred = predict( temp_mod, temp_df ), size_t0 = temp_df$size_t0 ) year_temp &lt;- paste0( design_df$yr_i[i] ) sp_temp &lt;- paste0( design_df$sp_i[i] ) plot_out &lt;- temp_df %&gt;% ggplot( aes( x = size_t0, y = size_t1 ) ) + geom_point( ) + labs( x = &quot;Size at t0&quot;, y = &quot;Size at t1&quot;, title = paste( &quot;Species&quot;, sp_temp, year_temp, sep = &quot; &quot; ) ) + geom_line( color = &#39;red&#39;, data = pred_df, aes( x = size_t0, y = size_t1_pred ) ) + geom_abline( slope = 1, intercept = 0, lty = 2 ) + theme_bw( ) return( plot_out ) } growth_plots &lt;- lapply( 1: 8, plot_growth ) wrap_plots( growth_plots ) + plot_layout( ncol = 2 ) Each function is evaluated in its own environment which is not the global environment. Use &lt;&lt;- to assign objects to the global environment within a function (object will appear in the list in the Environment pane). Use &lt;- to assign objects within the function’s environment (object will not appear in the Environment pane and will only exist when you call of the function). Functions can contain complex conditional syntax, i.e. chains of if else statements to perform different actions under certain circumstances. In general, a function should perform a single action; write multiple functions to string together instead of nesting multiple complex actions within the same function. "],["projorg.html", "2 Project Organization 2.1 Folder organization 2.2 More complicated datasets", " 2 Project Organization Each publication or dataset you work with will need its own R project to contain all of the work you will do to digitize the model and get it ready to be added to the PADRINO database. More information about selecting a publication to digitize and initializing a project can be found in Chapter 3 : General Workflow. All of your projects should be kept together in a folder. Therefore, it is important that you title your R projects consistently and descriptively. Generally, using the first author’s last name and the year of publication separated by an underscore is most appropriate. There may be some instances where appending more information, like the journal or the focal species, is necessary. Your folder of projects should be synced [SHARED CLOUD SERVICE]. This gives us all access to your scripts as you are working on them, which will be helpful when you are working remotely. 2.1 Folder organization Within the project home folder, you should create three subfolders: ‘R’, which will contain all of your scripts ‘data’, which will contain all data (more on this in a moment) ‘results’, which will contain any figures you create You may have other files that you want to keep in a project, such as a PDF of the full article text, supplemental files/appendices, etc. For the most part, these can also be kept in the project home folder, EXCEPT if they contain readable data files (Excel tables or .csv, etc). Any data files that you might use in your digitization should go into the ‘data’ folder, otherwise there is no reason to include them in the R project. Depending on the publication you’re working with, the amount of extra files you want to keep on-hand might be quite a lot. In that case, you should create a fourth subfolder titled ‘supplement’ and store everything in there. 2.1.1 R folder Scripts in the ‘R’ folder should be given names beginning with numbers indicating the order in which they should be executed. The first script will almost always be something like “01_DataFormatting.R” in which you will prepare the data you downloaded or otherwise extracted to get it ready for modeling. The rest of the scripts you might write are detailed in Chapter 3 : General Workflow. The file names suggested in that script can be modified by adding additional details about the dataset you are working with, if it helps you to better keep track of your work. Whatever you decide, please try to be consistent within each project! 2.1.2 Data folder The data folder contains both the files with the original data, and the output of formatting that same data. Always be sure to use descriptive names so you don’t mix up the raw data and the new data you worked so hard to put together! 2.1.3 Results folder Use the results folder to store the plots you need to make to check data before or during model fitting. To save yourself the hassle of recreating plots every time someone needs to take a look at them, export them each time you update them. 2.1.4 Example of folder organization  Author_year_species ↳  Author_year_species.Rproj  data ↳  raw_data.csv  survival_data.csv  growth_data.csv  recruitment_data.csv  R ↳ Ⓡ 01_DataFormatting.R Ⓡ 02_Plotting.R Ⓡ 03_VitalRates.R Ⓡ 04_IPMScratch.R Ⓡ 05_ipmr.R Ⓡ 06_PADRINO.R  results ↳  survival_plot.png  growth_plot.png  recruitment_plot.png  supplement ↳  Author_year_Appendix1.pdf  Author_year_Appendix2.pdf  temp ↳  test_data.csv  Author_year_fulltext.pdf  Author_year_species_pdb.xlsx Ⓡ Author_year_species_wrapper.R  Author_year_species_notes.txt 2.2 More complicated datasets For some datasets, the guidelines provided here are fully comprehensive. But some datasets may not be so simple, and they might require you to work on only a subset of the data at a time. In general, if it is a case where you are splitting the dataset into two subsets, or you are working with two slightly different models, it is fine to just add an appendix to your file names. Examples of when this is the case include: A model excluding the seedbank, and a model including the seedbank: 03_IPMScratch_noseedbank.R 03_IPMScratch_seedbank.R Two species (abbreviated with first 3 letters of genus &amp; species): 03_IPMScratch_Andger.R 03_IPMScratch_Boucur.R However, some datasets may necessitate the use of more than two different models or subsets. In these cases, it is best to create separate subfolders. Whether these subfolders go into the project home folder, or into the standard subfolders, it depends on the raw data. If you download more than one raw data file corresponding to each IPM (or set of IPMs), then the new subfolders should go into the project home folder. Ex.:  Author_year ↳  SpeciesA ↳  data ↳  raw_data_speciesA.csv  R  results  SpeciesB ↳  data ↳  raw_data_speciesB.csv  R  results  SpeciesB ↳  data ↳  raw_data_speciesB.csv  R  results Conversely, if the raw data are all contained within a single file (or multiple files all containing some kind of data for every model), then the subfolders should be nested within the standard subfolders. This is because you may have some scripts which apply to all versions of the model, like the first data formatting script, but then you will write different scripts for each version of the model that should be kept separate. Ex.:  Author_year ↳  data ↳  raw_data_all_species.csv  SpeciesA  SpeciesB  SpeciesC  R ↳ Ⓡ 01_DataFormatting.R  SpeciesA  SpeciesB  SpeciesC  results ↳  SpeciesA  SpeciesB  SpeciesC Ultimately, the decision to split into different subfolders and how to nest these folders is hard to make a priori. You might discover along the way that you need to simplify the model before tackling the whole thing, and you might end up making way more than two versions of a model. It is worthwhile to spend some extra time reorganizing your project if you think it will make it easier to understand. Finally, always be sure to explain any project structure/nestedness in the wrapper file (more details in Chapter 3 : General Workflow). "],["genwf.html", "3 General Workflow 3.1 Incoming candidates for digitization 3.2 Selecting a new project 3.3 Supply list 3.4 General workflow outline 3.5 Why do we construct the IPM twice?", " 3 General Workflow In the RUPDemo project, we will digitize IPMs from published studies, and directly from raw data. In the first scenario, you will recreate IPMs exactly how they are published in order to reproduce the exact same results from the paper, with the only difference being that you will reformat the model to be included within the PADRINO database. In the second scenario, you will completely parameterize an IPM from scratch based on a published dataset that lacks an IPM. Most of the “easiest” publications of the first type have already been added to PADRINO, leaving much more complicated datasets for us to work with. Most of these fall somewhere between the two types of digitization tasks, involving extensive data preparation and parameterization of vital rate models based on limited information reported in the publication. At the beginning of the RUPDemo project, we will all start with a collection of chart quadrat data that includes “the Adler dataset” and other similar datasets. Working with these datasets requires preparation of spatial data collected from chart quadrats and then complete parameterization of IPMs from scratch. Fortunately, I have already started digitizing some of the models from this collection and can supply a roadmap of the specific workflow for working with these data, which is found in Chapter 4. In the sections to follow, however, I will focus on outlining a more general workflow that can apply to pretty much any future RUPDemo digitization projects. This guide aims to amend what is currently missing from the official PADRINO digitization guide. Please refer to both of these guides in tandem to answer your questions; sections below which seem a little sparse are likely well-detailed in the official PADRINO guide. 3.1 Incoming candidates for digitization One aspect of the RUPDemo project is to identify datasets and publications that should be included in PADRINO. As a collaborative group, we all work to keep track of new IPMs coming in the literature. Roberto Salguero-Gomez (U. of Oxford) keeps track of the literature, and posts new articles containing IPMs in the PADRINO slack channel. The Leipzig team is in charge of reading up on posted literature and deciding if the article should be digitized or not. If it should be digitized, then it should be added to the the “Incoming literature” Google Sheet. Be sure to check the slack channel at least once a month, as messages are lost after 90 days. Publications posted in the slack channel may not actually be eligible for inclusion in PADRINO (usually because they do not have IPMs, or sometimes because they use MPMs instead). Be sure to reply to the message when you’ve checked a publication and let everyone know that it has been added to the Sheet, or otherwise tell us why it shouldn’t be added to the Sheet. You may come also across candidate publications “organically” while reading. If it seems like it may qualify for addition to PADRINO, go ahead and add it to the Sheet; no need to post about it on slack. If you’re unsure about whether a publication should be included, feel free to just ask me (or Aldo). 3.2 Selecting a new project After we finish with the Adler dataset, you might have the opportunity to pick what you will work on next. Within the scope of the RUPDemo project, we have some criteria to help you select your next project. Open up the Incoming literature Google Sheet (linked above) and explore your options according to these criteria: Kingdom: only “Plantae” We are only interested in digitizing IPMs for plants in the RUPDemo project. Digitized: only “N” Filter out any publications which have already been digitized or are in progress. Density dependence?: deselect “YES” For now, we will only focus on IPMs which do not account for density dependence. Please note that not all publications have been screened for density dependence, so you may come across some IPMs which do account for density dependence despite missing the “YES” in the density dependence column. If this is the case, please update the Sheet accordingly. Remark Take note of any comments from myself or other past digitizers about why this publication can not or should not be digitized, or essential information that is missing from the publication that currently prevents digitization. Length of study: ideally 10 transitions at minimum You will quickly notice that there are very few datasets with enough transitions at this point. Studies with adequate transitions have major roadblocks that prevent digitization, and the easiest models are built from very few transitions. You will likely need to choose to email the authors of one of the roadblocked datasets to ask for the missing information, and then hope they reply in a timely manner. If you cannot find anything to work on, ask me or Aldo and we can help you find something to tackle next. Once you have selected the publication you will work on next, be sure to update the “Digitized” column with “in progress” and add your initials to the “Digitizer” column to let everyone know that you’re working on it. If you’ve sent an email to the authors, then please put “Email” in the “Digitized” column. When you have finished digitizing, put “Y” in the “Digitized” column and celebrate your success! 3.3 Supply list Before you begin a digitization project, be sure you have all of the following supplies ready: Notebook to sketch out the IPM Microsoft Excel The following R packages: ggplot2 tidyverse patchwork lme4 readxl ipmr Rpadrino pdbDigitUtils testthat 3.4 General workflow outline More information on these steps are provided in the following chapters, or in the official PADRINO digitization guide linked previously. The PADRINO digitization guide has more detailed information about reformatting models for PADRINO, including explanations of each of the tables which need to be populated in the PADRINO database template Excel sheet. This guide focuses more on the workflow leading up to the final step of digitization (populating the database template). The script naming conventions used below are flexible and serve as an example of how many scripts you might need to produce for more complex digitization tasks. Simpler tasks where all parameter values are reported in the text may be digitized in only two scripts. 3.4.1 1. Read the publication Download a PDF of the article and track down all of the supplements and/or appendices (only download as necessary). Read the article, focusing mostly on the Materials &amp; Methods section; you can generally skim the other sections but be sure you are familiar with the whole text. Examine the supplements/appendices/supporting data and familiarize yourself with the information that is present, and think about what might be missing or what you might need to model. Start collecting the information that will go in the Metadata sheet of the PADRINO database template. 3.4.2 2. Sketch an outline of the IPM I find that it is helpful to draw a diagram of the different components of the IPM to identify which vital rates are modeled and how they fit into the kernels. At this stage, you can start to see which parameters you will need to track down or model and what types of models you will need to fit. Here is an example of an IPM sketch: In this model, the population consists of two stages: a continuous stage based on size which describes seedlings and adults, and a discrete stage describing the seedbank. The IPM for this population will include the P kernel describing the survival and growth of seedlings and adults, the F kernel describing the reproduction of adults (addition of seeds to the seedbank), a probability of a seed remaining in the seedbank, and a probability of germination of a seed out of the seedbank and into the continuous size class following a size distribution of new seedlings. In the sketch, we visualize all transition possibilities and consider which transitions are impossible (represented by zeros), which transitions are described by constants (discrete to discrete: probability of staying in the seedbank), which transitions are dependent on the continuous stage (continuous to continuous, e.g. survival and growth, or continuous to discrete, e.g. reproduction), and which transitions include size distributions (continuous to continuous, e.g. growth, or discrete to continuous, e.g. germination). Not all IPMs will include both continuous and discrete stages; simple IPMs only have a single continuous stage describing all transitions. Take the time to carefully consider all of the data you have to work with, and all of the possible transitions described by those data. Additionally, this is the stage to identify how many IPMs are actually included in the dataset and how they are related to one another, which can inform how you will organize your scripts &amp; subfolders and how many IPM IDs you will need to differentiate your models in the PADRINO database. We will assign a series of IPM IDs to you that you will pull from. Please contact us before moving on. I generally like to do this step together with Aldo; if you’d like to see how far you can get on your own first, that is perfectly fine, but these models are complex and often it takes more than one brain to ensure that everything is accounted for. 3.4.3 3. List the equations and parameters needed to recreate the IPM Once you feel like you’ve got a handle on how the IPM is put together, comb the text of the article to find out how all of the vital rates have been modeled. Write down the model expressions as they are presented in the text, and then consult Sam’s guide to Model expressions to rewrite the expressions in the correct format for ipmr. In this step, I like to create a “shopping list” of all of the parameters I will need to track down or model in order to fully recreate the IPM as published. This is also the step where you will identify if you will use suffixes in your parameters. 3.4.4 4. Take inventory Based on your “shopping list,” do you have all of the materials necessary to recreate the IPM? Identify where in the main text, supplements, appendices, and/or supporting data you will be able to find the information you need to construct each part of the IPM. Don’t just assume you’ll find it along the way! If you’re very lucky, the publication will actually report all vital rate model expressions, parameter values, and all of the necessary information you need to build each kernel. In this case, digitization can occur in a single R script (see steps 6 &amp; 11 and the digitization guide). In many cases, however, you will only have model expressions and raw data to build from. The steps 5 through 10 apply to these digitization projects. In some cases, you might find that only a few parameters are missing. This will require an email to the corresponding author from the PADRINO email (more information here). Be sure to take notes about what exactly is missing and note this in the Google Sheet so someone else doesn’t waste their time coming to the same conclusion you just did! 3.4.5 5. Create a new R project Once you know you have everything you need, create a new R project in your folder. Consult Chapter 2 for more details about structuring and organizing your project. In general, the project should be named according to the first author and the year of publication. 3.4.6 6. Script: 01_DataFormatting.R This first step of digitization can refer to either formatting the parameter values directly from the text before moving onto populating the database template (step 11), or formatting the raw data you’ve downloaded to prepare for model fitting (step 7). If the “raw” data you’re working with are actually the parameter estimates directly downloaded from the publication, then this step just involves putting the parameter estimates into their own dataframe with appropriate names for the vital rate models. In most cases, though, you will likely be working with actual raw data that needs to be cleaned up or reformatted. Follow what is written in the publication methods to make sure you are creating the subsets of data exactly as reported. The output of this script should be a couple of subsetted dataframes that are ready to be fit with linear models which describe the vital rates. This is also when you should start documenting any decisions you (or we) make about the dataset. Do you filter out any data? Why? Create a text document (Notepad, Word, R, etc.) and note down how you have filtered the data with justification for why. Store this document in the project home folder with a descriptive title, i.e. “Author_year_notes.txt” or “README.txt”. 3.4.7 7. Script: 02_PlotRawData.R In this script, you will plot the raw data that will be described by the vital rate models to check for any peculiarities in the data. Examples of what variables to plot include size_t1 ~ size_t0, survives_t1 ~ size_t0. Additionally, examine histograms of each variable. You should plot the data at multiple scales, including the smallest scale possible, i.e. one plot for each transition x treatment combination, and aggregated across all treatments or years. These plots should help you identify the distributions of the data as well as any individual datapoints that should be filtered out before model fitting. Be sure to document any further filtering that happens at this stage in the “notes” / “README” file. 3.4.8 8. Script: 03_VitalRateModels.R The next step is to fit linear models to the data to describe the vital rates. In some instances, it may not be clear exactly how the model should be fit; in this case, you may need to write multiple versions of the models and compare the AIC values of the different models to select the model with the best fit. Always plot your model results against data. This is relatively easy using the predict( ) function. In this stage, you might notice more peculiarities with the data that make it difficult to fit the model you might reasonably expect to use. Some models might do very well at predicting the entire dataset, but perform poorly at predicting within the different treatments/years/etc. Aldo will have suggestions for selecting the best model, which may include data transformations. Once you have selected the best fitting models for all of the vital rates, store all of the model parameters in the data folder as csv files. This will generally be four different files: one each for growth, survival, recruitment/reproduction, and “other,” which will include things like min and max sizes for the different kernels. It is useful to keep the different vital rates separate at this stage in case you need to go back to this script and make changes to only a single model. Again, document any data transformations, filtering/subsetting decisions, etc. in your notes document. 3.4.9 9. Script: 04_IPMScratch.R Finally it is time to construct the first IPM! In this script, you will build the IPM “from scratch,” writing functions to describe all of the vital rates and their link functions and then piecing them together in the kernel function. The first step is to put all of the vital rate parameters together into a list, where each list item is a named parameter value. Then, you will write all of the vital rate models as functions of x, y, and the input parameters (pars). Specific examples of how to write these functions will be provided in other documents. In general, you will need functions describing at least the following processes: standard deviation of growth, grow_sd( x, pars ) growth from size x to size y, gxy( x, y, pars ), which will return a probability density distribution describing growth for each x size class survival at size x, sx( x, pars ), which will describe the survival probability of each x size class the combined transition of survival * growth, pxy( x, y, pars ), which describes the P kernel, sx * gxy some sort of recruitment function(s), which may include size-dependent production of recruits fx( x, pars ) to describe the number of recruits produced by mothers of size x, then a size distribution of recruits recr( y, pars ) to return a probability density distribution describing the size of each recruit, and a function linking these together as fxy( x, y, pars ) constructed as fx * recr; or size-independent production of recruits (each mother produces the same number of recruits, recruitment is based on number of individuals in year t0), which is just a function of fy( y, pars ), returning a probability density distribution describing the size of new recruits More complex models may have additional function inputs, and these models do not necessarily describe all possible models you may need to write. Examples of how to structure these models will be provided as you are working, depending on what types of models you will work with. Once all of your vital rate functions are ready, you will then construct the function which generates the IPM kernel/matrix. The function should first define the domain of the IPM, which will stay the same across most models: n &lt;- pars$mat_siz # number of bins over which to # integrate L &lt;- pars$L # lower limit of integration U &lt;- pars$U # upper limit of integration h &lt;- ( U - L ) / n # bin size b &lt;- L + c( 0:n ) * h # lower boundaries of bins y &lt;- 0.5 * ( b[1:n] + b[2:( n + 1 )] ) # midpoints of bins # b are boundary points, # y are mesh points The next part of the kernel function should assemble the kernel matrices. Consult back to step 2 where you sketched out the IPM kernel to determine how many matrices and vectors you will need to assemble from the vital rate functions. Most IPMs will need an F matrix describing recruitment/fertility, and a T matrix describing yearly transitions of individuals, which is composed of the survival and growth matrices. You may also need to define a number of vectors which describe transitions in and out of discrete stages like a seed bank. Additionally, this section of the kernel function should correct for eviction, if necessary. The last part of the kernel function should put all of the matrices together which will look like k_yx &lt;- Tmat + Fmat in the case of the simplest model. End the function so that it returns all of the component matrices you might be interested in examining, in addition to the k_yx matrix. Finally, run your kernel function with the (mean) parameter values, and calculate the (mean) population growth rate, lambda: lambda &lt;- Re( eigen( kernel( pars )$k_yx )$value[1] ) If you’re working with a dataset that has a reported value for lambda, the population growth rate, compare the calculated lambda value(s) to the reported lambda value(s). If the results of the IPM seem unrealistic or very different from the reported lambda, you will need to troubleshoot before moving forward. This generally involves going back to the vital rate models to test out different data transformations or model formulas, but sometimes you might need to go all the way back to the data formatting step to make changes to the subsets of data you’re working with. If you’re parameterizing the IPM completely from scratch, then you will need to compare the calculated lambdas to the population numbers throughout the study period to check if the IPM is producing reasonable results. Depending on your dataset, you might also need to parameterize the IPM for a number of treatment levels or different years. Examples of how to approach this task will be provided as necessary. Describe your approach in the notes document. 3.4.10 10. Script: 05_ipmr.R Once you are confident that your from-scratch IPM is working correctly, you need to test that the IPM can be successfully recreated using the ipmr package and syntax, which is the underlying process powering PADRINO. There is a lot of helpful information which can be found on the ipmr site. Use the init_ipm( ) function in ipmr to start building your proto IPM, one kernel at a time. Reference the ipmr site and example scripts we provide to make sure you’re not forgetting any component. Start with only one kernel and the supplementary components to test to make sure each kernel is working independently before combining them. After defining each kernel, the init_ipm( ) function needs three more pieces to run: define_impl( ): define the implementation for each kernel This should be a list of lists, with the number of lists equal to the number of kernels and each nested list consisting of four components: kernel_name: name of the kernel (ex. \"P\", \"F\", \"SB_to_plant\") int_rule: for now, this will always be \"midpoint\" state_start: what is the name of the input variable for this kernel? state_end: what is the name of the output variable for this kernel? For simple IPMs, state_start and state_end will be the same variable which describes the continuous size class. For general IPMs, state_start and state_end are the same for the P and F kernels, as in simple IPMs, but they will be different for the kernels which describe transitions to and out of discrete states. For example, a kernel describing the size-dependent production of seeds going into a seedbank would have state_start = \"size\" and state_end = \"seedbank\" and the kernel describing the germination of seeds out of the seedbank and into the continuous size class would have state_start = \"seedbank\" and state_end = \"size\". define_domains( ): define the continuous domain(s) In this function, you need to tell ipmr all about the continuous state(s) that will be integrated. Each continuous state should have three values looking something like this: size = c( pars$L, # lower bound of the domain pars$U, # upper bound of the domain pars$mat_siz ) # number of meshpoints for the domain define_pop_state( ): define the initial state of the population Because ipmr computes everything through simulation, we must define a starting population vector that can be simulated from. All domains, both continuous AND discrete, need to have starting states. The below example describes a general IPM with a continuous domain describing size and a discrete domain describing a seedbank: define_pop_state( pop_vectors = list( n_size = rep( 1 / pars$mat_siz, pars$mat_siz ), n_seedbank = 1 / pars$mat_siz ) ) For simple IPMs with only a single continuous domain, n_size does not need to be contained in the pop_vectors list. It is essential that the n_ prefix is attached before the name of the variable describing the domain; make sure everything matches! Once all components of the proto IPM have been piped together in the init_ipm( ) function, you can then make the IPM and generate the lambda: make_ipm( name_of_proto_ipm, iterations = pars$mat_siz ) %&gt;% lambda( ) At this point, you might need to test the proto IPM for each treatment or year. Consult the example scripts or ask for help to write functions that will calculate the lambdas for each treatment/year/etc. Export these lambda values in a csv dataframe. Be sure to also export the parameter values which vary between years/treatments/etc., using an underscore to separate the name of the parameter and the specific year/treatment/etc. the parameter applies to. You should have three csv files in the end: parameter values which stay constant across all years/treatments/etc., parameter values which vary, and lambdas for each year/treatment/etc. Simpler datasets will only have a single csv output which describes all parameter values AND the lambda value. 3.4.11 11. Script: 06_PADRINO.R Now that you know the IPM can be recreated with the ipmr syntax, it is time to prepare the model to be included in PADRINO! Load all of the csvs which contain parameter values and lambdas for testing. Additionally, read in the Excel file of the PADRINO database template (replace the … with the path to wherever you’re storing the file!): sheet_names &lt;- excel_sheets(&quot;.../pdb_template.xlsx&quot;) pdb &lt;- lapply(sheet_names, function(x) { as.data.frame(read_excel(&quot;.../pdb_template.xlsx&quot;, sheet = x)) } ) names(pdb) &lt;- sheet_names Now, you can start populating each sheet of the template using common indexing syntax to access each sheet, such as pdb$Metadata[i,j]. This is also a good stage to write some functions to fill in all of the information for multiple IPM IDs, if you need to. Consult the PADRINO digitization guide for more details about which things go where: https://padrinodb.github.io/Padrino/digitization-guide.html#the-tables The pdb$VitalRateExpr sheet will likely be the most complicated. Check the examples often. The majority of the issues arise from inconsistencies in naming the different parameters, kernels, and vital rate expressions. If you capitalize the name of the parameter in the vital rate expression, be sure to also capitalize the name in the pdb$ParameterValues sheet!!! Once all of the information for every model is entered into the spreadsheet, export the file to Excel and test each IPM ID: write_xlsx( pdb,&quot;pdb_Author_year.xlsx&quot; ) pdb_test &lt;- read_pdb( &quot;pdb_Author_year.xlsx&quot; ) ipm_ids &lt;- pdb_test$Metadata$ipm_id output &lt;- c( ) test_ipms &lt;- function( i ){ output[i] &lt;- capture.output( test_model( pdb_test, ipm_ids[x] ) ) return( output ) } lapply( 1:length( ipm_ids ), test_ipms ) # replace IPM_ID below with a single ID to look at one IPM at a time pdb_test_proto &lt;- pdb_make_proto_ipm( pdb_test, det_stoch = &quot;det&quot; ) print( pdb_test_proto$IPM_ID ) test_ipm &lt;- make_ipm( pdb_test_proto$IPM_ID ) test_ipm lambda( test_ipm, type_lambda=&quot;all&quot; ) test_model( pdb_test,id = &quot;IPM_ID&quot;) In all likelihood, your IPM(s) won’t work. You will get some sort of cryptic error message that really doesn’t tell you what went wrong and you won’t have any idea where to begin troubleshooting. My biggest piece of advice for this step is to seriously take your time when populating the Excel sheet. It is very tempting to think that the hard part is over, the IPM works, now you just need to plug in some numbers into a spreadsheet, you’re so close to finishing this model! and then you rush through this step and the model fails to run from the Excel file and it is super frustrating. Nearly always, there is nothing actually wrong with the model. Some combination of two things has most likely happened: there is at least one typo or inconsistency in how you call the parameters/vital rates/kernels, and/or there is an issue with how some of the data are written into the Excel sheet OR read from the Excel sheet. The first thing to do when your model doesn’t run is to take a break. Have a coffee, step away from the screen, do something else for at least five minutes. Remind yourself that the hardest part IS done, the model DOES run, and you WILL find the problem. The next thing to do is to open the Excel file that contains the model and take a look at it in Excel. Browse through all of the sheets and look for anything that may have been exported wrong. Sometimes, it is as simple as your indexing was [j,i] when it was supposed to be [i,j], so one of the sheets is weirdly transposed. Maybe some of the columns don’t actually align correctly because the F kernel was repeated twice when it was supposed to be repeated three times. Problems like these are easy to fix in the script; re-export the Excel file and test it again. If all of the cells seem to be filled in the right configuration, the next thing to check is consistency in calling all of the parameters, vital rates, and kernels. Check the StateVariables, ContinuousDomains, IntegrationRules, StateVectors, IpmKernels, VitalRateExpr, ParameterValues, EnvironmentalVariables, and ParSetIndices sheets and make sure every name is consistent in spelling and capitalization. I often find that I catch a ton of little errors like this. While it is frustrating, these are also easy to fix and test again. Once you are absolutely certain that every cell contains the correct info, if the IPM still doesn’t work, it may be an issue with how the data are encoded. Think about how Excel is truly terrible at formatting dates: sometimes, numbers get interpreted as dates when they are absolutely not, or conversely, dates get interpreted as impossible points in time. Convincing Excel to interpret these values correctly is not always easy. I’ve come up with a number of columns that may get encoded or interpreted incorrectly, which can be bypassed by running these lines of code after you have loaded the Excel model, but before you begin the testing: pdb_test$Metadata$eviction_used &lt;- as.logical( pdb_test$Metadata$eviction_used ) pdb_test$ContinuousDomains$lower &lt;- as.numeric( pdb_test$ContinuousDomains$lower ) pdb_test$ContinuousDomains$upper &lt;- as.numeric( pdb_test$ContinuousDomains$upper ) pdb_test$StateVectors$n_bins &lt;- as.numeric( pdb_test$StateVectors$n_bins ) pdb_test$ParameterValues$parameter_value &lt;- as.numeric( pdb_test$ParameterValues$parameter_value ) pdb_test$TestTargets$precision &lt;- as.numeric( pdb_test$TestTargets$precision ) You might need to perform similar actions on other columns if these still don’t fix the issue. If after performing all of these checks the model still won’t run, ask for help. A second set of eyes might be able to pick out an error you missed, or maybe Aldo actually knows what the ipmr error message is referring to! Eventually, you will get the model(s) performing correctly and you can move on to the final step! 3.4.12 12. Final steps of digitization Store the completely populated PADRINO database Excel file in the project home folder. Update the notes file to make sure all of the decisions you made about the data and changes you made to the dataset are described in a reproducible way. Write a “wrapper” file which describes the project. This file should also be stored in the project home folder. The name of the wrapper file should be something like “ProjectName_overview.R” or “ProjectName_wrapper.R”. The wrapper file should include an introduction that describes the dataset (which species, when were the data collected, where were the data collected, and a full citation of the publication) and the purpose of the project (to recreate the published IPM for inclusion in the PADRINO database). Then, all of the scripts should be listed in the order in which they should be executed, with minimal description of the steps only if they are necessary. Briefly describe any other files contained within the project that have not been already mentioned. Take this time to also reorganize any other files that may have ended up in the project. If you produced multiple versions of any file for testing, create a new folder called ‘temp’ and store the extras in there. Essentially, the whole point of the wrapper file is to communicate to an outside person exactly what you did so that they could open any of the files in the project and know what they’re looking at. This is useful for yourself to revisit older projects in reference to starting new projects, but it is also essential to the entire team. PADRINO is a work in progress and we have lots of goals to improve the database. Some of these goals involve adding other components to the digitization process, so it would be incredibly helpful to be able to know exactly what you did in case we need to add to your projects in the future. Creating the wrapper file is not necessarily a step that must be saved until you’ve completed a project. You are of course free to create the wrapper at any stage of the project, especially if it helps you keep everything organized! I find that it is particularly helpful to already have a wrapper in progress if I know I am going to put a project aside for a few weeks to help me get back into the workflow. Make sure all of the files are organized, uploaded, and synced to whatever cloud service we’ve decided to use. Update the “Digitized” column in the Incoming literature Google Sheet to “Y” and let us know it is time to celebrate! 3.5 Why do we construct the IPM twice? At this point, you might be asking “Why did I write the IPM from scratch if I actually needed to write it using ipmr for PADRINO?” and the answer is that troubleshooting in the PADRINO format is much harder to do. Writing the IPM from scratch first gives you the ability to go back and troubleshoot exactly the part of these complex models that is causing an issue. In the PADRINO format, even identifying the part of the model that is “wrong” is very challenging and unintuitive. Therefore, constructing the IPM from scratch gives us more opportunities to catch and correct mistakes. "],["adwf.html", "4 Adler Dataset: Example of Workflow 4.1 Introduction to the Adler dataset 4.2 Publications featuring the Adler dataset 4.3 plantTracker 4.4 Workflow with example dataset", " 4 Adler Dataset: Example of Workflow At this point, we’ve planned out a roadmap for digitization in a general sense, but how do these steps actually happen in a real example? The following sections walk through the digitization of an IPM that Aldo and I worked on last year, using part of a dataset that you will soon become very familiar with! All of the code to reproduce the IPM is included, with extensive commentary to explain the different decisions we made along the way. 4.1 Introduction to the Adler dataset The Adler dataset is composed of a collection of chart quadrat data that was collected across the western United States in the early to mid twentieth century. Chart quadrat data consists of paper maps that track each individual of each plant species in a 1 square meter plot from census to census (usually yearly but not always). Grasses are mapped as polygons and forbs are mapped as points. Because of these methods, IPMs can only be used to model the population dynamics of the grasses; forbs must be modeled using age-structured matrix models. Chart quadrats census the whole community, but here we will focus on the species with roughly 100 individuals per year. Chart quadrat data are analyzed through the R package plantTracker to extract individual data on site, survival, and recruitment. We plan run plantTracker in advance, and to provide you with the data. We will use the chart quadrat dataset collected from Colorado as our example to learn how to work with these data. There are six other sites that will then each be assigned to a digitizer (including one from Moore et al.); other datasets may follow later on. For now, we formatted data for a total of 15 IPMs, and at least 4 MPMs. If you end up needing to use plantTracker at any point to process a new incoming chart quadrat dataset, we have some example scripts you can use in addition to the resources provided with the plantTracker package documentation: https://doi.org/10.1111/2041-210X.13950 https://github.com/aestears/plantTracker Digitizing models from the Adler dataset requires consulting with relevant publications which may have already parameterized models from these data, as well as modeling a bit at your own discretion with our guidance. The materials which follow should provide you with the tools to tackle the Adler dataset, but please don’t hesitate to ask for help! 4.2 Publications featuring the Adler dataset Once you have been assigned to a site or subset of the Adler dataset, you should first read the publications which have already worked with the data. Note that the list below should not be considered exhaustive; you might find other papers which use these data and it is your job to make sure you read any that might be relevant. Additionally, not all of the papers below will be relevant to all of the species at your assigned site, and some of them may not be relevant at all to creating IPMs/MPMs. The list is just to provide you with some context about the dataset and give you a place to begin understanding how to model these data. All sites: Chu et al., 2013, Journal of Vegetation Science https://doi.org/10.1111/jvs.12106 Stears et al., 2022, Methods in Ecology and Evolution https://doi.org/10.1111/2041-210X.13950 Chu &amp; Adler, 2015, Ecological Monographs https://doi.org/10.1890/14-1741.1 (except Colorado) Chu et al., 2016, Nature Communications https://doi.org/10.1038/ncomms11766 (except Colorado) Tredennick et al., 2018, Ecology Letters https://doi.org/10.1111/ele.13154 (except Colorado) Chu &amp; Adler, 2023, Journal of Ecology https://doi.org/10.1111/1365-2745.12212 (except Colorado) Colorado: Chu et al., 2013, Ecology https://doi.org/10.1890/13-0121.1 Stears et al., 2022, Ecology https://doi.org/10.1002/ecy.3799 Idaho: Adler et al., 2010, Ecology Letters https://doi.org/10.1111/j.1461-0248.2010.01496.x Zachmann et al., 2010, Ecology https://doi.org/10.1890/10-0404.1 Dalgleish et al., 2011, Ecology https://doi.org/10.1890/10-0780.1 Kansas: Adler et al., 2007, Ecology https://doi.org/10.1890/0012-9658(2007)88[2673:LMQFKP]2.0.CO;2 Lauenroth &amp; Adler, 2008, Journal of Ecology https://doi.org/10.1111/j.1365-2745.2008.01415.x Arizona: Anderson et al., 2012, Ecology https://doi.org/10.1890/11-2200.1 Montana: Anderson et al., 2011, Ecology https://doi.org/10.1890/11-0193.1 Tredennick et al., 2016, Methods in Ecology and Evolution https://doi.org/10.1111/2041-210X.12686 New Mexico: Christensen et al., 2021, Ecology https://doi.org/10.1002/ecy.3530 4.3 plantTracker The majority of the Adler dataset is available in its most raw form: digitized shapefiles of the original maps, and tabular data describing the shapefiles (as well as other information about the sites). The data must be transformed into a format that can be used to parameterize the vital rate models which we can do with the package plantTracker, developed by the Adler lab. Essentially, plantTracker overlays maps of each plot from successive timesteps and assigns unique identifiers to each individual based on their locations in the plot in order to track each individual throughout the monitored period. We can define a number of parameters to tell plantTracker how precisely polygons and points from successive years must overlap in order to be considered the same individual. Stears et al. (2022, Methods in Ecology &amp; Evolution) provides an excellent description of these parameters and how plantTracker works, if you are interested in reading more about it. For most of the Adler dataset, we will use the parameters described in the relevant publications (which I believe are also the default values). For the example dataset we will work through here (Colorado, focusing on Bouteloua gracilis), the call to use plantTracker looked like this: datTrackSpp &lt;- trackSpp( CO_grasses_raw_df, quad_inventory_list, dorm = 1, buff = 0.05, clonal = TRUE, buffGenet = 0.05, aggByGenet = TRUE, flagSuspects = TRUE ) Here, CO_grasses_raw_df was the raw dataframe containing all spatial information for all maps, filtered specifically to only include the grasses which had a minimum cover of 100, and quad_inventory_list included the metadata about each mapped quadrat. For the rest of the arguments: - dorm = 1: individuals are permitted to go dormant for one year and reappear in the same spot in year t2 as they appeared in year t0 - buff = 0.05: individuals are allowed to “move” no more than 5 cm between years, which accounts for minor errors in mapping and also true variation in where individuals are located from year to year - clonal = TRUE: single genetic individuals (genets) can be composed of multiple vegetative segments (ramets) distributed across multiple rows of the data - buffGenet = 0.05: polygons of the same species within 5 cm of each other can be considered part of the same genet - aggByGenet = TRUE: multiple rows referring to the same genet will be aggregated so that each genet is only described by a single row - flagSuspects = TRUE: suspect observations will be flagged; observations can be suspect if an individual shrinks more than 10%, or if an individual which went dormant in time t1 had an area of less than 0.05 in time t0 The output of plantTracker is a dataframe describing the survival (and size, for grasses) of each individual throughout its entire tracked period. We can also see in which year an individual was added to the population as a recruit and the age of the individuals in each year, except for the individuals which were present in the first year. ## Quad trackID Year basalArea_genet recruit survives_tplus1 age size_tplus1 nearEdge Suspect Treatment Pasture ## 1 gzgz_11 BOUGRA_1997_1 1997 0.4126254 NA 1 NA 0.4117354 TRUE FALSE gzgz 11 ## 2 gzgz_11 BOUGRA_1997_1 1998 0.4117354 NA 1 NA 0.3879517 TRUE FALSE gzgz 11 ## 3 gzgz_11 BOUGRA_1997_1 1999 0.3879517 NA 1 NA 0.3754913 TRUE FALSE gzgz 11 ## 4 gzgz_11 BOUGRA_1997_1 2000 0.3754913 NA 1 NA 0.4095949 TRUE FALSE gzgz 11 ## 5 gzgz_11 BOUGRA_1997_1 2001 0.4095949 NA 1 NA 0.1874513 TRUE FALSE gzgz 11 ## 6 gzgz_11 BOUGRA_1997_1 2002 0.1874513 NA 1 NA 0.3677826 TRUE FALSE gzgz 11 We can then use these data to parameterize vital rate models and then MPMs/IPMs, following the general workflow described in Chapter 3. Below, I will walk us through this workflow for our example species from Colorado, Bouteloua gracilis. 4.4 Workflow with example dataset 4.4.1 1. Reading the publications The Colorado dataset is featured in four of the publications linked above. In Stears et al. (2022, Methods in Ecology &amp; Evolution), the dataset is only mentioned as an example of the type of data which can be analyzed using plantTracker, so there is nothing new to extract from here. In the other three papers, Chu et al. (2013, Ecology), Chu et al. (2013, Journal of Ecology), and Stears et al. (2022, Ecology), details are given about how the data were collected, but there is little else to gain for our models. The one piece of information that might be useful is that the tracked plots are not all equal; there are two levels of a grazing experiment (historically grazed or ungrazed, and grazed or ungrazed during the sampling period, four total treatment combinations) which we should consider analyzing separately (see Chapter 5 ). 4.4.2 2. Sketching an outline of the IPM We are going to use this dataset to produce a single IPM based on all of the aggregated data for Bouteloua gracilis. In theory, however, this dataset could be exploited in a number of different ways. We have sufficient data to produce yearly models, and RUPDemo focuses on temporal replication (in order to focus on the effects of yearly climate). Additionally, each of the six tracked pastures has one plot of each treatment combination: grazed in the past + grazed now (gzgz), ungrazed in the past + grazed now (ungz), grazed in the past + ungrazed now (gzun), and ungrazed in the past + ungrazed now (unun). In theory, we could construct as many as 126 IPMs from these data: 1 IPM per transition, all plots aggregated (13 IPMs) 1 IPM using mean parameter values across all transitions and all plots (1 IPM) 1 IPM per transition, per treatment combination (13 x 4 = 52 IPMs) 1 IPM per treatment combination, using mean parameter values across all transitions (4 IPMs) 1 IPM per transition, per historical treatment, ignoring the current treatment (13 x 2 = 26 IPMs) 1 IPM per historical treatment, using mean parameter values across all transitions and ignoring the current treatment (2 IPMs) 1 IPM per transition, per current treatment, ignoring the historical treatment (13 x 2 = 26 IPMs) 1 IPM per current treatment, using mean parameter values across all transitions and ignoring the historical treatment (2 IPMs) However, we don’t have enough data for this many IPMs. Remember, we want roughly 100 individuals per year, per treatment! This table describes the number of individuals from Bouteloua gracilis that are present in each year and treatment combination: gzgz gzun ungz unun Total 1997 10 6 14 12 42 1998 16 8 15 19 58 1999 43 29 33 54 159 2000 43 5 20 16 84 2001 54 23 45 52 174 2002 123 22 65 724 934 2003 99 12 38 137 286 2004 118 18 53 94 283 2005 78 14 46 35 173 2006 67 10 104 52 233 2007 143 39 46 86 314 2008 134 50 64 94 342 2009 90 40 45 70 245 2010 71 29 53 67 220 Only a few treatment x year combinations have enough individuals for IPMs, and even if we aggregate all individuals across all treatments, three years have insufficient individuals! We could combine all yearly data, and produce four IPMs - one for each treatment. However, we will focus first on just constructing the mean IPM. In Chapters 5 through 7, we will explore some of these other options. Let’s take a look at our data to see what kinds of information we have to work with: ## Quad trackID Year basalArea_genet recruit survives_tplus1 age size_tplus1 nearEdge Suspect Treatment Pasture ## 1 gzgz_11 BOUGRA_1997_1 1997 0.4126254 NA 1 NA 0.4117354 TRUE FALSE gzgz 11 ## 2 gzgz_11 BOUGRA_1997_1 1998 0.4117354 NA 1 NA 0.3879517 TRUE FALSE gzgz 11 ## 3 gzgz_11 BOUGRA_1997_1 1999 0.3879517 NA 1 NA 0.3754913 TRUE FALSE gzgz 11 ## 4 gzgz_11 BOUGRA_1997_1 2000 0.3754913 NA 1 NA 0.4095949 TRUE FALSE gzgz 11 ## 5 gzgz_11 BOUGRA_1997_1 2001 0.4095949 NA 1 NA 0.1874513 TRUE FALSE gzgz 11 ## 6 gzgz_11 BOUGRA_1997_1 2002 0.1874513 NA 1 NA 0.3677826 TRUE FALSE gzgz 11 For each individual, we have information on the size at time t0 (basalareaGenet) and at time t1 (size_tplus1), whether the individual survives from t0 to t1 (survives_tplus1), and whether the individual is a recruit in a given year (NA meaning that the individual was present in the first year so we cannot know if it is a recruit or not, 0 meaning not a recruit, 1 meaning it is a recruit). From this information, we will be able to construct our three vital rate models. Our recruitment data is available at the plot-level, but we do not have any information about how many recruits are produced by a given individual. Therefore, our recruitment model must be constructed as the number of recruits produced per non-recruit (henceforth, let’s call these “adults”) at time t0. This means that “recruits” are not a discrete stage but rather will be included as part of the continuous matrix describing size at t0 and size at t1, with the only thing differentiating recruits from adult plants is that in their first year (the year they are recruited into the population), the recruits can have an arbitrarily small size at t0 (when they were recorded as points rather than polygons on the map). Our IPM will therefore be the sum of the P kernel, \\(P(z&#39;, z) = s(z) * G(z&#39;, z)\\), and the F kernel, \\(F(z&#39;) = f(z&#39;)\\), where \\(s(z)\\) represents the survival probability of an individual of size \\(x\\) at time t0, \\(G(z&#39;, z)\\) represents the transition from size \\(z\\) at time t0 to size \\(z&#39;\\) at time t1 (using a probability density distribution), and \\(f(z&#39;)\\) represents the size \\(z&#39;\\) of recruits at time t1 dependent on the number of non-recruits at time t0. 4.4.3 3. Making the parameter “shopping list” Given that we are building IPMs from data, we now need to estimate the parameters for the IPM submodels: the survival model \\(s(z)\\), the growth model, \\(G(z&#39;,z)\\), and the recruitment model, \\(F(z&#39;)\\). 4.4.3.1 Survival model Survival is measured as binary data (0 = did not survive to time t1, 1 = survived to time t1) so it is modeled with a generalized linear model which uses a binomial distribution and a logit “link function”. In mathematical notation, we can describe the survival model as: \\[Logit(s(z)) = \\beta_{s0} + \\beta_{s1} * z\\] where \\(z\\) is the size of an individual at time t0, \\(\\beta_{s0}\\) is the intercept and \\(\\beta_{s}\\) is the slope. To estimate \\(\\beta_{s0}\\) and \\(\\beta_{s1}\\) in R, we would model this as: glm( survives_t1 ~ size_t0, data = survival_df, family = binomial ) In ipmr, due to a technical issue, the model is written using the inverse logit equation: s = 1 / ( 1 + exp( -( beta_s0 + beta_s1 * size_0 ) ) ) The values for beta_s0 and beta_s1 will come from the model estimates from the R model, so these values will be added to our “shopping list.” 4.4.3.2 Growth model The growth model, which describes the transition in size of an individual from time t0 to time t1, relies on a Gaussian probability density function to generate \\(size_{t1}\\) (\\(z&#39;\\) in the IPM model) given a certain \\(size_{t0}\\) (\\(z\\) in the IPM). We can write the growth model in a more conventional notation to better align with how we will need to write the model for PADRINO: \\[G(z&#39;, z) = f_{g}(z&#39;, \\mu_{g}, \\sigma_{g})\\] \\[\\mu_{g} = \\beta_{g0} + \\beta_{g1} * z\\] \\[ \\sigma_{g} = \\sqrt{( a * e^{( b * z )} )}\\] where \\(f_{g}\\) denotes the Gaussian probability density function which outputs the size of an individual at time t1 (\\(z&#39;\\)) based on the size at time t0 (\\(z\\)). In R, we will model \\(\\mu_{g}\\) to obtain the values \\(\\beta_{g0}\\) and \\(\\beta_{g1}\\), and we will also model \\(\\sigma_{g}\\) using the nonlinear least-squares estimates to obtain the parameters which describe the variance of the growth model (where a is the intercept and b is the slope): lm( size_t1 ~ size_t0, data = growth_df ) nls( y ~ a * exp( b * x ), start = list( a = 1, b = 0 ) ) Once we have all of the parameter values, we will need to write the growth model in three parts for PADRINO: g = Norm( mu_g, sigma_g ) mu_g = beta_g0 + beta_g1 * size_1 sigma_g = sqrt( a * exp( b * size_0 ) ) In total, we are adding beta_g0, beta_g1, a, and b to our shopping list for the growth model. 4.4.3.3 Recruitment model As previously discussed, the recruitment model will express a size-independent per-capita rate of recruitment. In mathematical notation: \\[F(z&#39;) = \\beta_{f} * r_{d}(z&#39;)\\] \\[r_{d}(z&#39;) = f_{r_{d}}(z&#39;, \\mu_{r_{d}}, \\sigma_{r_{d}})\\] In R, we will write the model as: MASS::glm.nb( NRquad ~ 1, data = recr_df ) This will give us our estimate for \\(\\beta_{f}\\). The other estimates will come from the raw data. \\(\\beta_{f}\\) represents the mean per-capita recruitment rate, i.e. N_recruit_t1 / N_adult_t0 for each plot x year; \\(\\mu_{r_{d}}\\) represents the mean size of recruits in year t0 when they were added to the population; and \\(\\sigma_{r_{d}}\\) represents the standard deviation of the recruit sizes in year t0. The recruitment model will be formatted for PADRINO as: fy = beta_f * r_d r_d = Norm( recr_sz, recr_sd ) Therefore, we will add beta_f, recr_sz, and recr_sd to the “shopping list” for the recruitment model. 4.4.3.4 Other parameters In addition to the vital rate parameters, there are a few other parameters we will need to define in order to build our IPM in PADRINO. These include the minimum and maximum sizes over which we will integrate, as well as the number of bins that ipmr will use to solve the integral. We will calculate the minimum and maximum sizes from the raw data, and we will use 200 as the number of bins. 4.4.4 4. Taking inventory Based on our parameter shopping list we generated previously, we will need to calculate or generate estimates from linear models for all of our parameter values. If we were working with a published IPM, this is where we would consult the publication and supplemental materials to track down as many of these values as possible. 4.4.5 5. Creating a new R project Here is an example of how we might organize this R project, assuming that we will write IPMs for other species from the Colorado site:  Adler_Colorado ↳  Adler_Colorado.Rproj  data ↳  Bou_gra  R ↳  Bou_gra  results ↳  Bou_gra  supplement 4.4.6 6. Data formatting To fit models from the raw data, we will produce separate data frames for the survival, growth, and recruitment models. The original files will be given to you as either .csv or .rds files, which contain the output from plantTracker. For now, we will assume that we’re working with the csv containing only the data for Bouteloua gracilis, which we abbreviate as “Bou_gra”. Bou_gra &lt;- read.csv( &quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/Bou_gra.csv&quot; ) We also want to take a moment to check the spatiotemporal replication of the entire dataset, to see if all plots were sampled in every year, or if any plots/years were skipped during data collection. Aldo and I will have already done this before sending you the data for your project and we will let you know if there are any inconsistencies to keep in mind, but let’s briefly walk through how we check for sampling completion. The easiest way is to just plot every year x quadrat combination in a grid and check for gaps: file_quads &lt;- &#39;https://www.dropbox.com/scl/fi/vc5sg2fqj4kf5l9wbnjcj/quad_inventory.csv?rlkey=2arxy6cor1cwsbo4mul5z1dv1&amp;dl=1&#39; quad_inventory &lt;- read.csv( file_quads, sep = &#39;\\t&#39; ) %&gt;% pivot_longer( gzgz_11:unun_7, names_to = &#39;Quad&#39;, values_to = &#39;Year&#39; ) ggplot( quad_inventory, aes( x = Year, y = Quad ) ) + geom_point( ) In this plot, a point indicates that a certain quadrat was surveyed in the year indicated on the x axis. Each quadrat is described by its treatment combination, followed by a number which refers to the pasture it is in. No data exist for a handful of quadrats in the year 2000, for any species. These discontinuities will not have substantial effects on vital rate estimation other than dramatically changing sample size in the 1999-2000 and 2000-2001 transitions. However, these discontinuities will require care when estimating population growth rates, which we will need to check our models. This is because, without care, we might get a crash in population growth in 1999-2000, and an explosion in 2000-2001. Before we create our dataframes for each vital rate model, we need to update the 2009 survival data. Because we told plantTracker that plants can go dormant for 1 year, all individuals which were alive in 2009 but not recorded in 2010 (the final year of surveys) are given an “NA” value in the column describing survival to time t1. We’re going to go ahead and assume that these individuals all died, so those NA values need to be changed to zero. Bou_gra &lt;- Bou_gra %&gt;% mutate( survives_tplus1 = replace( survives_tplus1, is.na( survives_tplus1 ) &amp; Year == 2009, 0 ) ) We can then test this: Bou_gra %&gt;% group_by( Year ) %&gt;% summarise( prop = sum( survives_tplus1, na.rm = T ) / length( survives_tplus1 ) ) ## # A tibble: 14 × 2 ## Year prop ## &lt;int&gt; &lt;dbl&gt; ## 1 1997 0.857 ## 2 1998 0.672 ## 3 1999 0.415 ## 4 2000 0.798 ## 5 2001 0.753 ## 6 2002 0.301 ## 7 2003 0.493 ## 8 2004 0.530 ## 9 2005 0.676 ## 10 2006 0.652 ## 11 2007 0.739 ## 12 2008 0.705 ## 13 2009 0.686 ## 14 2010 0 Here we can see that the proportion of individuals which survive to time t1 in 2009 is not unrealistically higher than the other years, and since we remove the NAs in the calculation, the proportion of individuals in 2010 which survive to time t1 (2011) is zero, or otherwise completely unknown. Now we can create a subsetted dataframe containing the information we need for our survival vital rate model: surv &lt;- subset( Bou_gra, !is.na( survives_tplus1 ) ) %&gt;% subset( basalArea_genet != 0 ) %&gt;% select( Quad, Year, trackID, Treatment, Pasture, basalArea_genet, logsize, survives_tplus1, size_tplus1 ) Here we are taking only individuals which we definitively know about their survival in year t1, which means we exclude the final year of sampling data since we do not know what happened in the following year. We also excluded individuals which have size_t0 (basalArea_genet) equal to zero since these individuals may have been tracked incorrectly. Lastly, we select the columns which might be relevant to our survival model, including information about the year, treatment, individual ID, size, and survival. Next, we will create a subsetted dataframe containing the information we will need for our growth vital rate model: grow &lt;- Bou_gra %&gt;% subset( basalArea_genet != 0) %&gt;% subset( size_tplus1 != 0) %&gt;% select( Quad, Year, trackID, Treatment, Pasture, basalArea_genet, logsize, survives_tplus1, size_tplus1) Here we are excluding individuals which have size_t0 and size_t1 equal to zero (which died or recruited), and then selecting the same relevant columns. Originally we thought that we should calculate the recruitment rate based on the mean area occupied by adults at time t0, which meant that we needed to calculate the total area occupied by adults for each plot x year: quad_df &lt;- Bou_gra %&gt;% group_by( Species, Pasture, Quad, Year ) %&gt;% summarise( totParea = sum( basalArea_genet ) ) %&gt;% ungroup Then, we took the mean of these areas across each pasture x year: group_df &lt;- quad_df %&gt;% group_by( Species, Pasture, Year ) %&gt;% summarise( Gcov = mean( totParea ) ) %&gt;% ungroup Next, we put the cover data together into one dataframe to link cover at time t0 to the number of recruits at time t1, dropping NAs just in case there were still any left at this point: cover_df &lt;- left_join( quad_df, group_df ) %&gt;% mutate( Year = Year + 1 ) %&gt;% mutate( Year = as.integer( Year ) ) %&gt;% drop_na() However, further exploration of the data resulted in this information being irrelevant to our model. In the model we will actually work with, we only need to calculate the number of recruits for each plot x year: recr_df &lt;- Bou_gra %&gt;% group_by( Species, Pasture, Quad, Year ) %&gt;% summarise( NRquad = sum( recruit, na.rm=T ) ) %&gt;% ungroup The merged dataframes (dropping NAs which inevitably show up in NRquad) is what you will find in the Github, but just know that we will only work with the number of recruits per quad, NRquad, in recr: recr &lt;- left_join( cover_df, recr_df ) %&gt;% drop_na Lastly, we should write these three dataframes as csv files to be loaded into the subsequent scripts: write.csv( surv, &quot;data/Bou_gra/survival_df.csv&quot; ) write.csv( grow, &quot;data/Bou_gra/growth_df.csv&quot; ) write.csv( recr, &quot;data/Bou_gra/recruitment_df.csv&quot; ) 4.4.7 7. Plotting the raw data In this step, we will explore the raw data and make some decisions about excluding certain data points before we fit our vital rate models. Note that most figures are preceded by script which saves the figure to the ‘results’ folder in the R project; I have commented out this command because there is no need to run it in the Markdown file, but it is included to give you examples of how these plots could be named and which plots should be exported. Let’s first plot the raw data using histograms to examine the distributions of size at t0 and size at t1. # png( &#39;results/Bou_gra/histograms.png&#39;, width = 8, height = 3, units = &quot;in&quot;, res = 150 ) par( mfrow = c( 1, 2 ), mar = c( 3.5, 3.5, 1, 0.2 ), mgp = c( 2, 0.7, 0 ), cex = 0.8 ) hist( Bou_gra$basalArea_genet, main = &quot;Histogram of size at time t0&quot;, xlab = &quot;Size at time t0&quot; ) hist( Bou_gra$size_tplus1, main = &quot;Histogram of size at time t1&quot;, xlab = &quot;Size at time t1&quot; ) # dev.off( ) It is immediately apparent that the vast majority of individuals are very small, but that there are a few individuals which are much larger. The distribution for size at time t1 is nearly identical to that of size at time t0. These data are not normally distributed, which we can consider correcting for by log-transforming the sizes: # png( &#39;results/Bou_gra/histograms_log.png&#39;, width = 8, height = 3, units = &quot;in&quot;, res = 150 ) par( mfrow = c( 1, 2 ), mar = c( 3.5, 3.5, 1, 0.2 ), mgp = c( 2, 0.7, 0 ), cex = 0.8 ) hist( Bou_gra$logsize, main = &quot;Histogram of log-transformed size at time t0&quot;, xlab = &quot;log( Size at time t0 )&quot; ) hist( log( Bou_gra$size_tplus1 ), main = &quot;Histogram of log-transformed size at time t1&quot;, xlab = &quot;log( Size at time t1 )&quot; ) # dev.off( ) Log transformation approaches a more normal distribution, especially in size at time t1, but the data are still not perfectly described by the normal distribution. For now, we will move on to plotting the vital rates and then revisit the distributions of the data once we filter out some datapoints. Next, we want to plot all of the vital rates. In order to plot survival, we need to put the survival data into binned proportions. Since the survival data are binary, it is very hard to notice any patterns in the relationship between size_t0 and survival at t1 when the raw data are plotted, as the points just appear as solid lines at 0 and 1 on the y axis. But by binning the survival data into classes based on size_t0, we can examine the proportion of individuals in each size class which survive and therefore read the plot as “what is the relationship between size and probability of survival?” Let’s first look at all of the survival data: h &lt;- ( max( Bou_gra$logsize, na.rm = T ) - min( Bou_gra$logsize, na.rm = T ) ) / 20 lwr &lt;- min( Bou_gra$logsize, na.rm = T ) + ( h * c( 0:( 20 - 1 ) ) ) upr &lt;- lwr + h mid &lt;- lwr + ( 1/2 * h ) binned_prop &lt;- function( lwr_x, upr_x, response ){ id &lt;- which( Bou_gra$logsize &gt; lwr_x &amp; Bou_gra$logsize &lt; upr_x ) tmp &lt;- Bou_gra[id,] if( response == &#39;prob&#39; ){ return( sum( tmp$survives_tplus1, na.rm = T ) / nrow( tmp ) ) } if( response == &#39;n_size&#39; ){ return( nrow( tmp ) ) } } y_binned &lt;- Map( binned_prop, lwr, upr, &#39;prob&#39; ) %&gt;% unlist x_binned &lt;- mid y_n_size &lt;- Map( binned_prop, lwr, upr, &#39;n_size&#39; ) %&gt;% unlist surv_binned &lt;- data.frame( xx = x_binned, yy = y_binned, nn = y_n_size ) %&gt;% setNames( c( &#39;logsize&#39;, &#39;survives_tplus1&#39;, &#39;n_size&#39; ) ) # png( &#39;results/Bou_gra/survival_binned.png&#39;, width = 6, height = 4, units = &quot;in&quot;, res = 150 ) ggplot( data = surv_binned, aes( x = logsize, y = survives_tplus1 ) ) + geom_point( alpha = 1, pch = 16, size = 1, color = &#39;red&#39; ) + scale_y_continuous( breaks = c( 0.1, 0.5, 0.9 ) ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ) ) + labs( x = expression( &#39;log(size)&#39;[t0] ), y = expression( &#39;Survival to time t1&#39; ) ) # dev.off( ) Together, the data show a general trend of increasing probability of survival as log( size_t0 ) increases, which we would expect. Now we can take a look at the growth data: # png( &#39;results/Bou_gra/growth.png&#39;, width = 6, height = 4, units = &quot;in&quot;, res = 150 ) ggplot(data = grow, aes( x = logsize, y = log( size_tplus1 ) ) ) + geom_point( alpha = 0.5, pch = 16, size = 0.7, color = &#39;red&#39; ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ) ) + labs( x = expression( &#39;log( size )&#39;[t0] ), y = expression( &#39;log( size )&#39;[t1] ) ) # dev.off( ) The most obvious feature of this plot is the bold vertical line at the minimum of log( size_t0 ). These points represent individuals which were recruited into the population at time t0 (and were given an arbitrarily small area) and which grew to a larger size in time t1. More interesting are the points forming horizontal lines, which represent individuals which were of a decent size in time t0, but then regressed to the minimum size in year 1. It is possible that some of these individuals actually died after the census in t0 and a new seedling germinated in roughly the same place before the census in t1, but we really do not know for sure if this is the case for all of these individuals. Since it is possible that individuals regressed in size, we will keep these datapoints. Keep in mind the scale of the axes; since the sizes have been log-transformed, the largest size possible is 0, which corresponds to 1 m\\(^2\\). The rest of the plot appears more or less how we would expect: a positive relationship between size_t1 and size_t0, with some variation. Most individuals seem to fall between -9 (corresponding to 0.01 cm\\(^2\\)) and -3 (corresponding to 4 cm\\(^2\\)), which are all fairly small plants. It would not be unreasonable to assume that what appears from this plot as plants shrinking quite a lot from time t0 to time t1 are actually the result of measurement error. The only individuals we might consider filtering out are those with an “impossible” size, i.e. individuals whose size is smaller than the minimum size that we specified when assembling the plantTracker data. Now that we have an idea of how the size and survival data look, let’s take a look at the per-capita recruitment: # png( &#39;results/Bou_gra/recruit.png&#39;, width = 6, height = 4, units = &quot;in&quot;, res = 150 ) ggplot( recr, aes( x = totParea, y = NRquad ) ) + geom_point( alpha = 0.5, pch = 16, size = 1, color = &#39;red&#39; ) + theme_bw( ) + labs( x = expression( &#39;Total parent plant area&#39;[t0] ), y = expression( &#39;Number of recruits&#39;[t1] ) ) # dev.off( ) Immediately we notice an outlier which obscures our ability to examine the dataset as a whole. This is a single plot where 682 individuals were recruited into the plot in 2002. Let’s remove the outlier and take a look: # png( &#39;results/Bou_gra/recruit_nomax.png&#39;, width = 6, height = 4, units = &quot;in&quot;, res = 150 ) recr %&gt;% filter( NRquad != max( recr$NRquad ) ) %&gt;% ggplot( aes( x = totParea, y = NRquad ) ) + geom_point( alpha = 0.5, pch = 16, size = 1, color = &#39;red&#39; ) + theme_bw( ) + labs( x = expression( &#39;Total parent plant area&#39;[t0] ), y = expression( &#39;Number of recruits&#39;[t1] ) ) # dev.off( ) Now we can see that most plots have very few or even no recruits, and that there appears to be a negative relationship between total parent plant area at time t0 and the number of recruits at time t1. Biologically, we might explain this as negative density dependence, where adult plants are suppressing recruitment into their direct vicinity to avoid competition with members of the same species, or it might simply be that the space is already occupied and there is nowhere for recruits to germinate. In plots with low parent plant area, there might be more open ground where germination can occur. We will not filter out any data from the recruitment dataframe. We will, however, modify the handful of “impossibly small” individuals from the growth and survival data. These are individuals whose log( size ) at either t0 or t1 is less than the arbitrarily small size that was assigned to individuals that were recorded as points on the chart quadrat maps. For some reason, when these individuals were digitized, they were digitized as polygons that ended up with areas less than the arbitrarily small size (0.25 cm\\(^2\\), or -10.59663 on the log scale). There are three records with areas less than this size. Quad gzgz_24, BOUGRA_2003_53, 2003: This is a recruit in 2003 which dies before the census in 2004, so we will just change the size_t0: Bou_gra[which( Bou_gra$Quad == &quot;gzgz_24&quot; &amp; Bou_gra$trackID == &quot;BOUGRA_2003_53&quot; ),7] &lt;- 0.000025 Bou_gra[which( Bou_gra$Quad == &quot;gzgz_24&quot; &amp; Bou_gra$trackID == &quot;BOUGRA_2003_53&quot; ),16] &lt;- log( 2.5e-05 ) Quad gzgz_24, BOUGRA_2002_1, 2003: This plant recruited in 2002 but then shrank to below the minimum size in 2003, so we will need to change size_t0 in 2003 and size_t1 in 2002: Bou_gra[which( Bou_gra$Quad == &quot;gzgz_24&quot; &amp; Bou_gra$trackID == &quot;BOUGRA_2002_1&quot; &amp; Bou_gra$Year == &quot;2003&quot; ),7] &lt;- 0.000025 Bou_gra[which( Bou_gra$Quad == &quot;gzgz_24&quot; &amp; Bou_gra$trackID == &quot;BOUGRA_2002_1&quot; &amp; Bou_gra$Year == &quot;2003&quot; ),16] &lt;- log( 2.5e-05 ) Bou_gra[which( Bou_gra$Quad == &quot;gzgz_24&quot; &amp; Bou_gra$trackID == &quot;BOUGRA_2002_1&quot; &amp; Bou_gra$Year == &quot;2002&quot; ),11] &lt;- 0.000025 Quad ungz_24, BOUGRA_2006_7, 2006: This is a recruit in 2006 which entered the population with a size below the minimum size, so we will only need to change size_t0 in 2006: Bou_gra[which( Bou_gra$Quad == &quot;ungz_24&quot; &amp; Bou_gra$trackID == &quot;BOUGRA_2006_7&quot; &amp; Bou_gra$Year == &quot;2006&quot; ),7] &lt;- 0.000025 Bou_gra[which( Bou_gra$Quad == &quot;ungz_24&quot; &amp; Bou_gra$trackID == &quot;BOUGRA_2006_7&quot; &amp; Bou_gra$Year == &quot;2006&quot; ),16] &lt;- log( 2.5e-05 ) Once we perform these changes (which we should be recording in our notes file, by the way!), we should re-run the Data Formatting script on the modified Bou_gra dataframe and save new versions of the vital rate dataframes. Add a suffix to the file names like “_mod” or “_clean” when you write the new csvs to indicate that these files have the modified datapoints, and so you don’t write over the original versions in case you want to check something else out later! 4.4.8 8. Fitting vital rate models for the mean IPM Now that we have an idea of how our data are distributed and we have modified the occurrences that are not possible, we are ready to start fitting our vital rate models. Let’s first do a bit more reformatting of the dataframes (after we’ve modified those individuals as mentioned in the previous step) to make our models more consistent: grow_df &lt;- grow %&gt;% mutate( logarea.t0 = log( basalArea_genet ), logarea.t1 = log( size_tplus1 ) ) surv_df &lt;- surv %&gt;% mutate( logarea = log( basalArea_genet ) ) 4.4.8.1 Growth model Now we can start by modelling growth, using the simplest model to describe the effect of size at time t0 on size at time t1. gr_mod_mean &lt;- lm( logarea.t1 ~ logarea.t0, data = grow_df) Let’s check how well the mean model predicts the whole dataset. grow_df$pred &lt;- predict( gr_mod_mean, type = &quot;response&quot; ) grow_line &lt;- ggplot( grow_df, aes( x = logarea.t0, y = logarea.t1 ) ) + geom_point( ) + geom_abline( aes( intercept = coef( gr_mod_mean )[1], slope = coef( gr_mod_mean )[2] ), color = &#39;red&#39;, lwd = 2 ) grow_pred &lt;- ggplot( grow_df, aes( x = logarea.t1, y = pred ) ) + geom_point( ) + geom_abline( aes( intercept = 0, slope = 1 ), color = &quot;red&quot;, lwd = 2 ) # png( &#39;results/Bou_gra/grow_pred.png&#39;, width = 10, height = 8, units = &quot;in&quot;, res = 150 ) grow_line + grow_pred + plot_layout( ) # dev.off( ) The plot on the left shows the raw data with the model plotted over as a line, which seems to fit to the data more or less fine. The plot on the right shows the observed sizes at time t1 on the x axis and the sizes at time t1 predicted by the model on the y axis, with a 1:1 line plotted over the data. The model seems to slightly overestimate growth for smaller individuals, and slightly underestimate growth for the largest individuals. Recall back to our “shopping list” that we constructed in step 3. Our growth model requires a slope and an intercept, which we can get from the output of the mean model, but also estimates of “a” and “b,” which will come from the variance model: x &lt;- fitted( gr_mod_mean ) y &lt;- resid( gr_mod_mean )^2 gr_var_m &lt;- nls( y ~ a * exp( b * x ), start = list( a = 1, b = 0 ) ) We’ll get back to these later. For now, let’s move on to the survival model. 4.4.8.2 Survival model This is the simplest model describing the effect of size at time t0 on survival to time t1: su_mod_mean &lt;- glm( survives_tplus1 ~ logarea, data = surv_df, family = &quot;binomial&quot; ) Let’s quickly plot the data and see how well the model fits: surv_x &lt;- seq( min( surv_df$logarea ), max( surv_df$logarea ), length.out = 100) surv_pred &lt;- boot::inv.logit( coef( su_mod_mean )[1] + coef( su_mod_mean )[2] * surv_x ) surv_pred_df &lt;- data.frame( logarea = surv_x, survives_tplus1 = surv_pred ) surv_line &lt;- ggplot( ) + geom_jitter( data = surv_df, aes( x = logarea, y = survives_tplus1 ), alpha = 0.25, width = 0, height = 0.25 ) + geom_line( data = surv_pred_df, aes( x = logarea, y = survives_tplus1 ), color = &#39;red&#39;, lwd = 2 ) surv_bin &lt;- ggplot( ) + geom_point( data = surv_binned, aes( x = logsize, y = survives_tplus1 ) ) + geom_line( data = surv_pred_df, aes( x = logarea, y = survives_tplus1 ), color = &#39;red&#39;, lwd = 2 ) # png( &#39;results/Bou_gra/survival_pred.png&#39;, width = 10, height = 8, units = &quot;in&quot;, res = 150 ) surv_line + surv_bin + plot_layout( ) # dev.off( ) On the left, I’ve used geom_jitter( ) and alpha = 0.25 to help with the visualization of the raw data but it’s still hard to tell how well the mean model fits the data. On the right, I’ve plotted the binned data we looked at previously and now we can see that the mean model seems to fit pretty well. 4.4.8.3 Recruitment model Moving on to the recruitment model, remember we are modeling a per-capita rate of recruitment based on the number of adults in time t0, and we currently have the number of recruits at time t1 and the area occupied by adults at time t0 for each plot. First, we model the number of recruits using the negative binomial distribution: rec_mod_mean &lt;- MASS::glm.nb( NRquad ~ 1, data = recr_df ) Next, we use the predict( ) function to predict the number of recruits per quad, and then we sum up the observed and predicted number of recruits across all quads: recr_df &lt;- recr_df %&gt;% mutate( pred_mod_mean = predict( rec_mod_mean, type = &quot;response&quot; ) ) rec_sums_df_m &lt;- recr_df %&gt;% summarize( NRquad = sum( NRquad ), pred_mod_mean = sum( pred_mod_mean ) ) Then, we sum up the number of adults present across all time t0: indiv_m &lt;- surv_df %&gt;% summarize( n_adults = n( ) ) Finally, we join these two dataframes and calculate a per-capita recruitment rate based on our model and on the observed recruits: repr_pc_m &lt;- indiv_m %&gt;% bind_cols( rec_sums_df_m ) %&gt;% mutate( repr_pc_mean = pred_mod_mean / n_adults ) %&gt;% mutate( repr_pc_obs = NRquad / n_adults ) %&gt;% drop_na And now we can compare the observed per-capita recruitment rate to the modeled per-capita recruitment rate: repr_pc_m ## n_adults NRquad pred_mod_mean repr_pc_mean repr_pc_obs ## 1 3327 1624 1624 0.4881274 0.4881274 The values are exactly the same, as we would expect. We will calculate the mean and standard deviation of the recruit sizes in the next step. 4.4.8.4 Exporting parameter estimates Now that we have all of our vital rate models, we need to store the coefficients from these models into dataframes containing our parameter estimates for the IPM. Starting with the growth model: grow_fe &lt;- data.frame( coefficient = names( coef( gr_mod_mean ) ), value = coef( gr_mod_mean ) ) var_m &lt;- data.frame( coefficient = names( coef( gr_var_m ) ), value = coef( gr_var_m ) ) grow_out &lt;- Reduce( function(...) rbind(...), list( grow_fe, var_m ) ) %&gt;% mutate( coefficient = as.character( coefficient ) ) %&gt;% mutate( coefficient = replace( coefficient, grepl( &quot;Intercept&quot;, coefficient ), &quot;b0&quot; ) ) # write.csv( grow_out, &quot;data/Bou_gra/grow_pars.csv&quot;, row.names = F ) And then the survival model: surv_fe &lt;- data.frame( coefficient = names( coef( su_mod_mean ) ), value = coef( su_mod_mean ) ) surv_out&lt;- Reduce( function(...) rbind(...), list( surv_fe ) ) %&gt;% mutate( coefficient = as.character( coefficient ) ) %&gt;% mutate( coefficient = replace( coefficient, grepl( &quot;Intercept&quot;, coefficient ), &quot;b0&quot; ) ) # write.csv( surv_out, &quot;data/Bou_gra/surv_pars.csv&quot;, row.names = F ) We also have some other parameters we need to gather, including the per-capita recruitment rate which we will store as fecu_b0: recSize &lt;- Bou_gra %&gt;% subset( recruit == 1) others &lt;- data.frame( coefficient = c( &quot;rec_siz&quot;, &quot;rec_sd&quot;, &quot;max_siz&quot;, &quot;min_siz&quot;, &quot;fecu_b0&quot; ), value = c( mean( log( recSize$basalArea_genet ) ), sd( log( recSize$basalArea_genet ) ), grow_df$logarea.t0 %&gt;% max, grow_df$logarea.t0 %&gt;% min, repr_pc_m$repr_pc_mean ) ) # write.csv( others, &quot;data/Bou_gra/other_pars.csv&quot;, row.names = F ) Now, we are finally ready to start building the IPM! 4.4.9 9. Building the IPM from scratch We will first construct a dataframe containing all of our mean IPM parameters: extr_value &lt;- function(x, field){ subset( x, coefficient == field )$value } pars_mean &lt;- list( surv_b0 = extr_value( surv_out, &quot;b0&quot; ), surv_b1 = extr_value( surv_out, &quot;logarea&quot; ), grow_b0 = extr_value( grow_out, &quot;b0&quot; ), grow_b1 = extr_value( grow_out, &quot;logarea.t0&quot; ), a = extr_value( grow_out, &quot;a&quot; ), b = extr_value( grow_out, &quot;b&quot; ), fecu_b0 = extr_value( others, &quot;fecu_b0&quot; ), recr_sz = extr_value( others, &quot;rec_siz&quot; ), recr_sd = extr_value( others, &quot;rec_sd&quot; ), L = extr_value( others, &quot;min_siz&quot; ), U = extr_value( others, &quot;max_siz&quot; ), mat_siz = 200 ) pars &lt;- pars_mean # write.csv( pars_mean, &quot;data/Bou_gra/pars_mean.csv&quot;, row.names = F ) Next, we need to write functions to describe each vital rate. Let’s start with growth. Recall that our growth function was modeled as: lm( logarea.t1 ~ logarea.t0, data = grow_df ) nls( y ~ a * exp( b * x ), start = list( a = 1, b = 0 ) ) And written in mathematical notation, as: \\[G(z&#39;, z) = f_{g}(z&#39;, \\mu_{g}, \\sigma_{g})\\] \\[\\mu_{g} = \\beta_{g0} + \\beta_{g1} * z\\] \\[ \\sigma_{g} = \\sqrt{( a * e^{( b * z )} )}\\] We need to write two functions: one which describes the standard deviation based on the input size at time t0, \\(x\\), and one which describes the transition from \\(x\\) to the size at time t1, \\(y\\), using a probability density distribution where the mean is described by our growth model. We can do that as such: # Function describing standard deviation of growth model grow_sd &lt;- function( x, pars ) { pars$a * ( exp( pars$b * x ) ) %&gt;% sqrt } # Function describing growth from size x to size y gxy &lt;- function( x, y, pars ) { return( dnorm( y, mean = pars$grow_b0 + pars$grow_b1*x, sd = grow_sd( x, pars ) ) ) } Now, we need to write a function for survival. Recall the survival model: glm( survives_tplus1 ~ logarea, data = surv_df, family = &#39;binomial&#39; ) And written in mathematical notation: \\[Logit(s(z)) = \\beta_{s0} + \\beta_{s1} * z\\] For survival, we only need to write one function (relying on the inverse logit function). This function describes the survival probability of an individual given its size \\(x\\). inv_logit &lt;- function( x ) { exp( x ) / ( 1 + exp( x ) ) } sx &lt;- function( x, pars ) { return( inv_logit( pars$surv_b0 + pars$surv_b1 * x ) ) } Now, we will link these functions together to describe the P kernel, \\(P(z&#39;, z) = s(z) * G(z&#39;, z)\\), describing the transition of an individual at time t0 to time t1 based on size at time t0, \\(x\\). pxy &lt;- function( x, y, pars ) { return( sx( x, pars ) * gxy( x, y, pars ) ) } Lastly, we will build our recruitment function. Recall that our recruitment model is size-independent, so there is no \\(z\\) input, but recruits have an associated size distribution, so the output of the function is expressed in terms of \\(z&#39;\\). This function is built after the mathematical notation: \\[f(z&#39;) = \\beta_{f} * r_{d}(z&#39;)\\] \\[r_{d}(z&#39;) = f_{r_{d}}(z&#39;, \\mu_{r_{d}}, \\sigma_{r_{d}})\\] However, both expressions will be accounted for in the same function: fy &lt;- function( y, pars, h ){ n_recr &lt;- pars$fecu_b0 recr_y &lt;- dnorm( y, pars$recr_sz, pars$recr_sd ) * h recr_y &lt;- recr_y / sum( recr_y ) f &lt;- n_recr * recr_y return( f ) } Now that we have our vital rate functions, we are ready to assemble everything into the IPM kernel/matrix function: kernel &lt;- function( pars ) { n &lt;- pars$mat_siz # number of bins over which to integrate L &lt;- pars$L # lower limit of integration U &lt;- pars$U # upper limit of integration h &lt;- ( U - L ) / n # bin size b &lt;- L + c( 0:n ) * h # lower boundaries of bins y &lt;- 0.5 * ( b[1:n] + b[2:( n + 1 )] ) # midpoints of bins # Fertility matrix Fmat &lt;- matrix( 0, n, n ) Fmat[] &lt;- matrix( fy( y, pars, h ), n, n ) # Survival matrix Smat &lt;- c( ) Smat &lt;- sx( y, pars ) # Growth matrix Gmat &lt;- matrix( 0, n, n ) Gmat[] &lt;- t( outer( y, y, gxy, pars ) ) * h # Growth/survival transition matrix Tmat &lt;- matrix( 0, n, n ) # Correct for eviction of offspring for( i in 1:( n / 2 ) ) { Gmat[1,i] &lt;- Gmat[1,i] + 1 - sum( Gmat[,i] ) Tmat[,i] &lt;- Gmat[,i] * Smat[i] } # Correct eviction of large adults for( i in ( n / 2 + 1 ):n ) { Gmat[n,i] &lt;- Gmat[n,i] + 1 - sum( Gmat[,i] ) Tmat[,i] &lt;- Gmat[,i] * Smat[i] } # Full Kernel is simply a summation of fertility and transition matrices k_yx &lt;- Fmat + Tmat return( list( k_yx = k_yx, Fmat = Fmat, Tmat = Tmat, Gmat = Gmat, meshpts = y ) ) } And we can use this function to calculate the mean population growth rate, \\(\\lambda_{mean}\\): lambda_ipm &lt;- function( i ) { return( Re( eigen( kernel( i )$k_yx )$value[1] ) ) } lam_mean &lt;- lambda_ipm( pars_mean ) lam_mean ## [1] 1.092364 Now, let’s check the observed population growth rate (changes in number of individuals) against the projected population growth rate. The population growth rate should be more precise than asymptotic lambda in checking our models. Asymptotic lambda is the population growth rate at the “stable stage distribution” - the stage distribution that the population reaches after a long period of time. # Load the cleaned data Bou_gra_clean &lt;- read.csv(&quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/Bou_gra_clean.csv&quot;) # Population counts at time t0 pop_counts_t0 &lt;- Bou_gra_clean %&gt;% group_by( Year, Quad ) %&gt;% summarize( n_t0 = n( ) ) %&gt;% ungroup %&gt;% mutate( Year = Year + 1 ) ## `summarise()` has grouped output by &#39;Year&#39;. You can override using the `.groups` argument. # Population counts at time t1 pop_counts_t1 &lt;- Bou_gra_clean %&gt;% group_by( Year, Quad ) %&gt;% summarize( n_t1 = n( ) ) %&gt;% ungroup ## `summarise()` has grouped output by &#39;Year&#39;. You can override using the `.groups` argument. # Calculate observed population growth rates, # accounting for discontinued sampling! pop_counts &lt;- left_join( pop_counts_t0, pop_counts_t1 ) %&gt;% # by dropping NAs, we remove gaps in sampling! drop_na %&gt;% group_by( Year ) %&gt;% summarise( n_t0 = sum( n_t0 ), n_t1 = sum( n_t1 ) ) %&gt;% ungroup %&gt;% mutate( obs_pgr = n_t1 / n_t0 ) ## Joining with `by = join_by(Year, Quad)` # Geometric mean of yearly population growth rates lam_mean_count &lt;- exp( mean( log( pop_counts$obs_pgr ), na.rm = T ) ) lam_mean_count ## [1] 1.164344 # Overall (aggregated) population growth rate lam_mean_overall &lt;- sum( pop_counts$n_t1 ) / sum( pop_counts$n_t0 ) lam_mean_overall ## [1] 1.05844 Once we are satisfied with our IPM, we can move on to constructing it with ipmr! 4.4.10 10. Building the IPM with ipmr Now we can move on to constructing the IPM using the ipmr syntax on which the PADRINO database relies. I will write out each component of the ipmr init_ipm( ) function independently, then run them together at the end. First, we initialize the init_ipm( ) function: library( ipmr ) proto_ipm_P &lt;- init_ipm( sim_gen = &quot;simple&quot;, di_dd = &quot;di&quot;, det_stoch = &quot;det&quot; ) %&gt;% The first part of the function asks us to define three characteristics of the proto-IPM: Simple, or general?: Simple IPMs have a single continuous state variable, while general IPMs have one or more continuous state variables and/or discrete stages. Since our model only has a single continuous state variable (size), our proto-IPM is “simple.” Density independent, or density dependent?: Our model is density independent, “di”. Deterministic, or stochastic?: Our model is deterministic, “det”. Next, we define the first kernel: define_kernel( name = &quot;P&quot;, family = &quot;CC&quot;, formula = s * g, s = plogis( surv_b0 + surv_b1 * size_1 ), g = dnorm( size_2, mu_g, grow_sig ), mu_g = grow_b0 + grow_b1 * size_1, grow_sig = sqrt( a * exp( b * size_1 ) ), data_list = pars_mean, states = list( c( &#39;size&#39; ) ), evict_cor = TRUE, evict_fun = truncated_distributions( fun = &#39;norm&#39;, target = &#39;g&#39; ) ) %&gt;% Here, we must define six characteristics of the kernel: Name: Give the kernel a name (derived from “F” for reproduction or “P” for survival/growth transition, but you can use other letters as necessary for more complex models). Family: What kind of transition does this kernel describe? The first letter denotes the type of state variable of the input to the kernel, and the second letter denotes the type of state variable of the output to the kernel. “C” indicates a continuous state variable, while “D” indicates a discrete state variable. For simple IPMs like ours, this will always be “CC”. Formula: How do the vital rates generate the sub-kernel? For our P kernel, the sub-kernel is the product of the survival and growth vital rates. Vital rate expressions: Now we must define all of the vital rates which we just wrote into the formula. Recall back to when we sketched out the IPM and wrote out the vital rate models in three different forms! In the case of our survival model, s, this is just the inverse of the link function of our survival GLM. In the case of our growth model, g, we must write three expressions. The first describes the Gaussian probability density distribution from which size at time t1 (which we denote as “size_2”) is drawn; the second defines \\(\\mu_{g}\\) based on size at time t0 (“size_1”) which will be input into the probability density distribution; and the third defines \\(\\sigma_{g}\\) based on size_1. Since the Gaussian distribution takes the identity link function, there is no need to define the inverse of the link function. The non-linear least squares estimate for \\(\\sigma_{g}\\) must be transformed as the square root. Specify parameters: Here, we must tell ipmr where to find the values of the parameters we just defined and assign them to the data_list. In this case, it is our mean parameters dataframe. We need to also define the state variable(s), which in our case is just “size”. Eviction correction: If the model corrects for eviction (which ours does), we need to define which functions should be corrected and the method used to correct for eviction. In our case, it is only the growth function that should be corrected following the normal distribution (for now), and the only method that is currently supported by ipmr is the truncated_distributions( ) option. Next, we need to define the implementation of our proto-IPM: define_impl( make_impl_args_list( kernel_names = c( &quot;P&quot; ), int_rule = rep( &quot;midpoint&quot;, 1 ), state_start = rep( &quot;size&quot;, 1 ), state_end = rep( &quot;size&quot;, 1 ) ) ) %&gt;% We use the helper function make_impl_args_list( ) in ipmr to define four components for implementation of the proto-IPM: Kernel names: List the names of the kernels that we’ve defined as a vector. Integration rule: Currently, the only option is the “midpoint” rule, which should be repeated for each kernel that we’ve defined. State start: Which state variable starts as the input to each kernel? In simple IPMs, this will always be the continuous state variable. State end: Which state variable ends as the output to each kernel? Again, this is the continuous state variable in simple IPMs like ours. Now we need to define the domains and the initial state of the population: define_domains( size = c( pars_mean$L, pars_mean$U, pars_mean$mat_siz ) ) %&gt;% define_pop_state( n_size = rep( 1 / 200, 200 ) ) To define the domains, we need to give ipmr three parameters that describe the continuous state variable. The first value is the lower bound of the domain, i.e. the lower limit of integration. The second value is the upper bound of the domain, i.e. the upper limit of integration. The third value is the number of meshpoints for the domain, which is 200 in our case. Then, because ipmr computes everything through simulation and this requires an input population state, we define the initial state of the population based on the number of meshpoints, i.e. assuming an equal number of individuals in each bin. Now, we can put everything together and implement the proto-IPM! library( ipmr ) proto_ipm_p &lt;- init_ipm( sim_gen = &quot;simple&quot;, di_dd = &quot;di&quot;, det_stoch = &quot;det&quot; ) %&gt;% define_kernel( name = &quot;P&quot;, family = &quot;CC&quot;, formula = s * g, s = plogis( surv_b0 + surv_b1 * size_1 ), g = dnorm( size_2, mu_g, grow_sig ), mu_g = grow_b0 + grow_b1 * size_1, grow_sig = sqrt( a * exp( b * size_1 ) ), data_list = pars_mean, states = list( c( &#39;size&#39; ) ), evict_cor = TRUE, evict_fun = truncated_distributions( fun = &#39;norm&#39;, target = &#39;g&#39; ) ) %&gt;% define_impl( make_impl_args_list( kernel_names = c( &quot;P&quot; ), int_rule = rep( &quot;midpoint&quot;, 1 ), state_start = rep( &quot;size&quot;, 1 ), state_end = rep( &quot;size&quot;, 1 ) ) ) %&gt;% define_domains( size = c( pars_mean$L, pars_mean$U, pars_mean$mat_siz ) ) %&gt;% define_pop_state( n_size = rep( 1 / 200, 200 ) ) ipmr_p &lt;- make_ipm( proto_ipm = proto_ipm_p, iterations = 200 ) lambda( ipmr_p ) ## lambda ## 0.866924 plot( ipmr_p ) Without the recruitment kernel, the survival/growth transition kernel produces a lambda that is less than one. Let’s add the recruitment kernel and see how that lambda changes. I will not add commentary here, as everything new follows the same logic as in the first proto-IPM. proto_ipm_pf &lt;- init_ipm( sim_gen = &quot;simple&quot;, di_dd = &quot;di&quot;, det_stoch = &quot;det&quot; ) %&gt;% define_kernel( name = &quot;P&quot;, family = &quot;CC&quot;, formula = s * g, s = plogis( surv_b0 + surv_b1 * size_1), g = dnorm( size_2, mu_g, grow_sig ), mu_g = grow_b0 + grow_b1 * size_1, grow_sig = sqrt( a * exp( b * size_1 ) ), data_list = pars_mean, states = list( c( &#39;size&#39; ) ), evict_cor = TRUE, evict_fun = truncated_distributions( fun = &#39;norm&#39;, target = &#39;g&#39; ) ) %&gt;% define_kernel( name = &#39;F&#39;, family = &#39;CC&#39;, formula = fecu_b0 * r_d, r_d = dnorm( size_2, recr_sz, recr_sd ), data_list = pars_mean, states = list( c( &#39;size&#39; ) ), evict_cor = TRUE, evict_fun = truncated_distributions( &quot;norm&quot;, &quot;r_d&quot; ) ) %&gt;% define_impl( make_impl_args_list( kernel_names = c( &quot;P&quot;, &quot;F&quot; ), int_rule = rep( &quot;midpoint&quot;, 2 ), state_start = rep( &quot;size&quot;, 2 ), state_end = rep( &quot;size&quot;, 2 ) ) ) %&gt;% define_domains( size = c(pars_mean$L, pars_mean$U, pars_mean$mat_siz ) ) %&gt;% define_pop_state( n_size = rep( 1 / 200, 200 ) ) ipmr_pf &lt;- make_ipm( proto_ipm = proto_ipm_pf, iterations = 200 ) lam_mean_ipmr &lt;- lambda( ipmr_pf ) lam_mean_ipmr ## lambda ## 1.099794 plot( ipmr_pf ) Mean lambda Value From scratch 1.0924 From ipmr 1.0998 These lambda values are very close! We can go ahead and move onto the penultimate step. 4.4.11 11. Populating the PADRINO database template Now that we have everything we need to construct our IPM using the ipmr syntax, we are ready to prep the IPM for inclusion in the PADRINO database. Be sure to have the pars_mean dataframe as well as the lambda value from the mean model loaded and ready to pull values from. First, we need to load in the Excel file of the empty template. Download the template from here and store it locally. Paste in the path to this script: library( readxl ) YOUR_PATH &lt;- &quot;this is where you should paste the path to the folder where you&#39;ve stored your file&quot; sheet_names &lt;- excel_sheets( paste( YOUR_PATH, &quot;/pdb_template.xlsx&quot;, sep = &quot;&quot; ) ) pdb &lt;- lapply( sheet_names, function( x ) { as.data.frame( read_excel( paste( YOUR_PATH, &quot;/pdb_template.xlsx&quot;, sep = &quot;&quot; ), sheet = x ) ) } ) names( pdb ) &lt;- sheet_names The pdb file contains 13 sheets, which are explained in better detail in the PADRINO digitization guide. We can get an overview of the different sheets with the View( pdb ) function. Currently, each sheet should be a dataframe with 0 rows and 3 - 47 columns that are waiting to be filled. I will use standard indexing to populate each of the relevant cells in the pdb file. It may not be the most efficient way to do this, but I find that it is easier to track down and correct mistakes. First, we should populate the Metadata table. This table will contain information about the dataset which you will have already collected in steps 1 through 4. You may have to consult back with the data paper or other publications to fill in some of the missing info. Consult the PADRINO digitization guide to help you understand what information goes in each column (but note that the template has changed since this guide was written, so check the column names and other example pdb files for further clarification). For the sake of our example, the IPM ID of this dataset will be “bg0000”. I like to create one long vector for the Metadata table, instead of filling each cell individually, since the information will be contained on a single row. pdb$Metadata[1,] &lt;- c( &quot;bg0000&quot;, # Taxonomic information &quot;Bouteloua gracilis&quot;, &quot;Bouteloua gracilis&quot;, &quot;Bouteloua&quot;, &quot;Poaceae&quot;, &quot;Poales&quot;, &quot;Liliopsida&quot;, &quot;Magnoliophyta&quot;, &quot;Plantae&quot;, &quot;Herbaceous&quot;, &quot;Monocot&quot;, &quot;angio&quot;, # Publication information &quot;Chu; Norman; Flynn; Kaplan; Lauenroth; Adler&quot;, &quot;Ecology&quot;, &quot;2013&quot;, &quot;10.1890/13-0121.1&quot;, &quot;Adler&quot;, &quot;peter.adler@usu.edu (2023)&quot;, NA, &quot;Chu, C., Norman, J., Flynn, R., Kaplan, N., Lauenroth, W.K. and Adler, P.B. (2013), Cover, density, and demographics of shortgrass steppe plants mapped 1997–2010 in permanent grazed and ungrazed quadrats. Ecology, 94: 1435-1435. https://doi.org/10.1890/13-0121.1&quot;, &quot;https://doi.org/10.6084/m9.figshare.c.3305970.v1&quot;, # Data collection information 14, 1997, NA, 2010, NA, 1, &quot;Shortgrass Steppe LTER&quot;, &quot;6&quot;, &quot;40.84519843&quot;, &quot;-104.7107395&quot;, &quot;1652.2&quot;, &quot;USA&quot;, &quot;n_america&quot;, &quot;TGS&quot;, # Model information &quot;A&quot;, TRUE, &quot;truncated_distributions&quot;, &quot;P; F&quot;, NA, FALSE, FALSE, FALSE, FALSE, &quot;&quot;, &quot;&quot;, &quot;&quot; ) pdb$Metadata$eviction_used &lt;- as.logical(pdb$Metadata$eviction_used) pdb$Metadata$duration &lt;- as.numeric(pdb$Metadata$duration) pdb$Metadata$periodicity &lt;- as.numeric(pdb$Metadata$periodicity) Some cells are case-sensitive, and some cells must be certain data types in order for ipmr to correctly parse the information. In general, copy what I or other digitizers have done. It also doesn’t hurt to run lines like the last line to make sure e.g. logical columns are stored as logical. Next we will populate the StateVariables table. This table will also only contain one row, describing the single continuous state variable size. In general IPMs, this table may have multiple rows, one for each continuous or discrete state variable. pdb$StateVariables[1,] &lt;- c( &quot;bg0000&quot;, &quot;size&quot;, FALSE) pdb$StateVariables$discrete &lt;- as.logical( pdb$StateVariables$discrete ) As with the StateVariables table, the ContinuousDomains table also only contains a single row to describe our single state variable. This is the first table where we are using values from our pars_mean dataframe to define the limits of integration over our continuous domain. Notice that I am filling the cells that should be kept blank with “” instead of NA. pdb$ContinuousDomains[1,] &lt;- c( &quot;bg0000&quot;, &quot;size&quot;, &quot;&quot;, pars_mean$L, pars_mean$U, &quot;P; F&quot;, &quot;&quot; ) pdb$ContinuousDomains$lower &lt;- as.numeric( pdb$ContinuousDomains$lower ) pdb$ContinuousDomains$upper &lt;- as.numeric( pdb$ContinuousDomains$upper ) Next is the IntegrationRules sheet. Again, this is a single row describing how we will integrate over our single continuous state variable. pdb$IntegrationRules[1,] &lt;- c( &quot;bg0000&quot;, &quot;size&quot;, &quot;&quot;, pars_mean$mat_siz, &quot;midpoint&quot;, &quot;P; F&quot; ) pdb$IntegrationRules$n_meshpoints &lt;- as.numeric( pdb$IntegrationRules$n_meshpoints ) Then the StateVectors sheet. This is where we tell ipmr how to initiate the population state for the simulation. pdb$StateVectors[1,] &lt;- c( &quot;bg0000&quot;, &quot;n_size&quot;, pars_mean$mat_siz, &quot;&quot; ) pdb$StateVectors$n_bins &lt;- as.numeric( pdb$StateVectors$n_bins ) Now we are getting to the more complicated tables. The IpmKernels sheet will have one row for each kernel we define, so two in the case of this IPM. Adding “* d_size” tells ipmr to integrate over the kernel with respect to size. pdb$IpmKernels[1,] &lt;- c( &quot;bg0000&quot;, &quot;P&quot;, &quot;P = s * g * d_size&quot;, &quot;CC&quot;, &quot;size&quot;, &quot;size&quot; ) pdb$IpmKernels[2,] &lt;- c( &quot;bg0000&quot;, &quot;F&quot;, &quot;F = fy * d_size&quot;, &quot;CC&quot;, &quot;size&quot;, &quot;size&quot; ) Next, we need to define all of our vital rate expressions which compose our kernels, using the proper syntax. All probability density functions should have “Substituted” in the column model_type and all other expressions should be “Evaluated”. If necessary, long expressions can be split onto multiple rows, i.e. one row for the inverse logit link function for the survival vital rate and one row describing the inside of the expression. Note that you do not need to define the state variable in probability density functions. pdb$VitalRateExpr[1,] &lt;- c( &quot;bg0000&quot;, &quot;Survival&quot;, &quot;s = 1 / ( 1 + exp( -( surv_b0 + surv_b1 * size_1 ) ) )&quot;, &quot;Evaluated&quot;, &quot;P&quot; ) pdb$VitalRateExpr[2,] &lt;- c( &quot;bg0000&quot;, &quot;Growth&quot;, &quot;mu_g = grow_b0 + grow_b1 * size_1&quot;, &quot;Evaluated&quot;, &quot;P&quot; ) pdb$VitalRateExpr[3,] &lt;- c( &quot;bg0000&quot;, &quot;Growth&quot;, &quot;g = Norm( mu_g, sd_g )&quot;, &quot;Substituted&quot;, &quot;P&quot; ) pdb$VitalRateExpr[4,] &lt;- c( &quot;bg0000&quot;, &quot;Growth&quot;, &quot;sd_g = sqrt( a * exp( b * size_1 ) )&quot;, &quot;Evaluated&quot;, &quot;P&quot; ) pdb$VitalRateExpr[5,] &lt;- c( &quot;bg0000&quot;, &quot;Fecundity&quot;, &quot;fy = fecu_b0 * r_d&quot;, &quot;Evaluated&quot;, &quot;F&quot; ) pdb$VitalRateExpr[6,] &lt;- c( &quot;bg0000&quot;, &quot;Fecundity&quot;, &quot;r_d = Norm( recr_sz, recr_sd )&quot;, &quot;Substituted&quot;, &quot;F&quot; ) Next, we need to define values for all of the parameters we just wrote in our vital rate expressions. These values go into the ParameterValues sheet. pdb$ParameterValues[1,] &lt;- c( &quot;bg0000&quot;, &quot;Survival&quot;, &quot;size&quot;, &quot;surv_b0&quot;, pars_mean$surv_b0 ) pdb$ParameterValues[2,] &lt;- c( &quot;bg0000&quot;, &quot;Survival&quot;, &quot;size&quot;, &quot;surv_b1&quot;, pars_mean$surv_b1 ) pdb$ParameterValues[3,] &lt;- c( &quot;bg0000&quot;, &quot;Growth&quot;, &quot;size&quot;, &quot;grow_b0&quot;, pars_mean$grow_b0 ) pdb$ParameterValues[4,] &lt;- c( &quot;bg0000&quot;, &quot;Growth&quot;, &quot;size&quot;, &quot;grow_b1&quot;, pars_mean$grow_b1 ) pdb$ParameterValues[5,] &lt;- c( &quot;bg0000&quot;, &quot;Growth&quot;, &quot;size&quot;, &quot;a&quot;, pars_mean$a ) pdb$ParameterValues[6,] &lt;- c( &quot;bg0000&quot;, &quot;Growth&quot;, &quot;size&quot;, &quot;b&quot;, pars_mean$b ) pdb$ParameterValues[7,] &lt;- c( &quot;bg0000&quot;, &quot;Fecundity&quot;, &quot;size&quot;, &quot;fecu_b0&quot;, pars_mean$fecu_b0 ) pdb$ParameterValues[8,] &lt;- c( &quot;bg0000&quot;, &quot;Fecundity&quot;, &quot;size&quot;, &quot;recr_sz&quot;, pars_mean$recr_sz ) pdb$ParameterValues[9,] &lt;- c( &quot;bg0000&quot;, &quot;Fecundity&quot;, &quot;size&quot;, &quot;recr_sd&quot;, pars_mean$recr_sd ) pdb$ParameterValues$parameter_value &lt;- as.numeric( pdb$ParameterValues$parameter_value ) For this IPM, we will skip the EnvironmentalVariables, ParSetIndices, and UncertaintyTable tables. The last table we will fill is the TestTargets table, with a single row. pdb$TestTargets[1,] &lt;- c( &quot;bg0000&quot;, &quot;lambda&quot;, lam_mean_ipmr, 3 ) pdb$TestTargets$target_value &lt;- as.numeric( pdb$TestTargets$target_value ) pdb$TestTargets$precision &lt;- as.numeric( pdb$TestTargets$precision ) Once all of the necessary sheets have been filled, export the pdb file to Excel, storing it in the project home folder. library( writexl ) write_xlsx( pdb, &quot;Bou_gra_pdb.xlsx&quot; ) Now, we must test the pdb file to make sure ipmr can recreate the model from the Excel tables. library( pdbDigitUtils ) pdb_test &lt;- read_pdb( &quot;Bou_gra_pdb.xlsx&quot; ) pdb_test_proto &lt;- pdb_make_proto_ipm( pdb_test, det_stoch = &quot;det&quot; ) print( pdb_test_proto$bg0000 ) bg_ipm_pdb &lt;- make_ipm( pdb_test_proto$bg0000 ) bg_ipm_pdb lambda( bg_ipm_pdb ) test_model( pdb_test, id = &quot;bg0000&quot; ) This is where we may start to see error messages which are difficult to interpret. In order to get this model to pass the test, I had to troubleshoot multiple tables and add several lines of code to make sure all of the necessary columns were stored as numeric or as logical values. Hopefully, by using this script as an example, you will not need to spend much time troubleshooting! 4.4.12 12. Finishing up the digitization project We have already saved the completely populated PADRINO database Excel file in the project home folder, and the name we gave the file is sufficient to differentiate it from other IPMs we might create from the Adler Colorado dataset. To share this Excel file outside of the home folder, it might be best to name it “Chu_2013_Bou_gra.xlsx” but for now, the name it has is fine. Next, we should finalize our notes file. This can be a .txt file or an .R script, if you’d like. The notes file for this project, named “Bou_gra_notes.txt”, should contain the following information: Only a single, simple, deterministic, density-independent IPM was constructed from these data, representing the mean model. All data were aggregated by treatment and by year to fit the vital rate models. In the year 2000, a number of quadrats were not sampled. These included unun_7, unun_5b, unun_24, unun_11, ungz_5a, ungz_24, gzun_5b, gzun_5a, gzun_24, gzgz_7, gzgz_5b, and gzgz_5a. Three individuals were observed to have sizes smaller than the minimum size. The sizes of these individuals were changed to the minimum size when the observed size was less than the minimum. These individuals were quad gzgz_24 BOUGRA_2003_53 in 2003 (size t0), quad gzgz_24 BOUGRA_2002_1 in 2003 (size_t0 in 2003, size_t1 in 2002) and quad ungz_24 BOUGRA_2006_7 in 2006 (size t0 in 2006). The recruitment rate was calculated as the mean per-capita rate of recruitment, based on the number of adults observed at time t0 and the number of recruits observed at time t1. Now, we need to write the wrapper file. First, we should take a look at how our files have been organized up to now. The project folder should look something like this:  Adler_Colorado ↳  Adler_Colorado.Rproj  data ↳  Bou_gra ↳  Bou_gra.csv  survival_df.csv  growth_df.csv  recruitment_df.csv  Bou_gra_clean.csv  survival_df_mod.csv  growth_df_mod.csv  recruitment_df_mod.csv  grow_pars.csv  surv_pars.csv  other_pars.csv  pars_mean.csv  R ↳  Bou_gra ↳ Ⓡ 01_DataFormatting.R Ⓡ 02_Plotting.R Ⓡ 03_VitalRates.R Ⓡ 04_IPMScratch.R Ⓡ 05_ipmr.R Ⓡ 06_PADRINO.R  results ↳  Bou_gra ↳  histograms.png  histograms_log.png  survival_binned.png  growth.png  recruit.png  recruit_nomax.png  grow_pred.png  survival_pred.png  recruit_pred.png  Chu_2013_fulltext.pdf  Bou_gra_pdb.xlsx  Bou_gra_notes.txt Ⓡ Bou_gra_wrapper.R Based on this, our wrapper file should contain the following information: # Chu et al. 2013: Bouteloua gracilis IPM for PADRINO # Aspen Workman, summer 2024 # Setup library( bblme ) # ver 1.0.25 library( ggplot2 ) # ver 3.4.4 library( htmlTable ) # ver 2.4.2 library( ipmr ) # ver 0.0.7 library( lme4 ) # ver 1.1-33 library( patchwork ) # ver 1.1.2 library( pdbDigitUtils ) # ver 0.0.0.90 library( readxl ) # ver 1.4.2 library( tidyverse ) # ver 2.0.0 library( writexl ) # ver 1.4.2 base_dir &lt;- dirname( rstudioapi::getActiveDocumentContext()$path ) R_dir &lt;- paste( base_dir, &quot;/R/Bou_gra/&quot;, sep = &quot;&quot; ) setwd( base_dir ) # Overview --------------------------------------------------------------------- # This R project concerns the construction of IPMs based on chart quadrat data # from Chu et al., 2013. This file focuses on the dataset of Bouteloua # gracilis, which was converted from chart quadrat data to demographic data by # A. Workman and A. Compagnoni using plantTracker. The dataset consists of 13 # transitions from 24 plots in Colorado, USA. A single, simple, deterministic, # density-independent IPM was constructed from these data, representing the # mean model. This IPM was formatted for inclusion in the PADRINO database. # Citation of data source: # Chu, C., Norman, J., Flynn, R., Kaplan, N., Lauenroth, W.K. and Adler, P.B. # (2013), Cover, density, and demographics of shortgrass steppe plants mapped # 1997–2010 in permanent grazed and ungrazed quadrats. Ecology, 94: 1435-1435. # https://doi.org/10.1890/13-0121.1 # Project organization --------------------------------------------------------- # Contained within the R project are all of the files needed to construct and # test multiple IPMs. The files which refer to the mean model for Bouteloua # gracilis are indicated by the abbreviation &quot;Bou_gra.&quot; The home folder of the # project contains the finalized PADRINO database Excel file # (&quot;Bou_gra_pdb.xlsx&quot;), a text file describing decisions that were made about # the data during the modeling process (&quot;Bou_gra_notes.txt&quot;) and this wrapper # file. # Data are stored in the &#39;data/Bou_gra&#39; subfolder. The raw data, &quot;Bou_gra.csv&quot;, # is the output from plantTracker filtered for only occurrences of Bouteloua # gracilis. The steps to generate the other data files in this subfolder are # contained within the R scripts. # The R scripts are stored in the &#39;R/Bou_gra&#39; subfolder. The number prefix of # the file name indicates the order in which the scripts should be executed. # Plots which were created throughout the course of this project are stored in # the &#39;results/Bou_gra&#39; subfolder. Plots with the &quot;_pred&quot; suffix in the file # name indicate visualizations of the vital rate models, while all other plots # are visualizations of the raw data. # Workflow --------------------------------------------------------------------- # Format the raw data source( paste0( R_dir, &quot;01_DataFormatting.R&quot; ) ) # Plot the raw data and clean the raw data source( paste0( R_dir, &quot;02_Plotting.R&quot; ) ) # Fit vital rate models source( paste0( R_dir, &quot;03_VitalRates.R&quot; ) ) # Construct IPM from scratch source( paste0( R_dir, &quot;04_IPMScratch.R&quot; ) ) # Construct IPM using ipmr syntax source( paste0( R_dir, &quot;05_ipmr.R&quot; ) ) # Populate the PADRINO database template source( paste0( R_dir, &quot;06_PADRINO.R&quot; ) ) Now that we’ve finished the wrapper file and all of the files are organized, we should make sure the entire project is uploaded or otherwise synced to whatever cloud service we’ve decided to use. The last step is to update the “Digitized” column in the Incoming literature Google Sheet to “Y” and celebrate our success! "],["adex.html", "5 Adler Dataset: Extensions to the Mean IPM 5.1 Exploring our options 5.2 Plotting by year x treatment", " 5 Adler Dataset: Extensions to the Mean IPM In Chapter 3, we followed the workflow to produce the mean IPM from our example dataset, Bouteloua gracilis from the Colorado site of the Adler dataset. However, this dataset had the potential to create as many as 125 other IPMs which we did not explore! This document aims to show how we constructed IPMs for each specific yearly transition, and how we might approach incorporating different treatments into our IPMs. This document assumes you have already read through Chapter 3, and only includes steps which deviate from the workflow described previously. If you are working with a similar dataset with hierarchical structure/nestedness, it would be a good idea to work out of a different subfolder in the overall R project to differentiate the mean model workflow from different workflows exploring levels of nestedness in the data. 5.1 Exploring our options First, recall the 126 possible IPMs we could build from the Bouteloua gracilis dataset. We have 13 transitions (14 years) of data, across six pastures, each of which contained four plots with unique grazing treatments. In theory, we could construct the following IPMs: 1 IPM per transition, all plots aggregated (13 IPMs) 1 IPM using mean parameter values across all transitions and all plots (1 IPM) ← we did this in Chapter 4 1 IPM per transition, per treatment combination (13 x 4 = 52 IPMs) 1 IPM per treatment combination, using mean parameter values across all transitions (4 IPMs) 1 IPM per transition, per historical treatment, ignoring the current treatment (13 x 2 = 26 IPMs) 1 IPM per historical treatment, using mean parameter values across all transitions and ignoring the current treatment (2 IPMs) 1 IPM per transition, per current treatment, ignoring the historical treatment (13 x 2 = 26 IPMs) 1 IPM per current treatment, using mean parameter values across all transitions and ignoring the historical treatment (2 IPMs) We could take this even further by incorporating the six different pastures as an added level of complexity; however, this random effect would certainly not have sufficient data to consider, so we will simply treat the different pastures as replicates of the different treatments. Let’s also take a look again at this table, which describes the number of observations in each treatment for each year: gzgz gzun ungz unun Total 1997 10 6 14 12 42 1998 16 8 15 19 58 1999 43 29 33 54 159 2000 43 5 20 16 84 2001 54 23 45 52 174 2002 123 22 65 724 934 2003 99 12 38 137 286 2004 118 18 53 94 283 2005 78 14 46 35 173 2006 67 10 104 52 233 2007 143 39 46 86 314 2008 134 50 64 94 342 2009 90 40 45 70 245 2010 71 29 53 67 220 Total 1089 305 641 1512 3547 Only a few treatment x year combinations have enough individuals for IPMs, and when we aggregate all individuals across all treatments, most years have sufficient individuals to construct IPMs (if only barely). Additionally, if we aggregate all individuals across all years for each treatment combination then all treatment combinations have sufficient individuals to construct IPMs. From our list above, we can then consider constructing 21 more IPMs: 1 IPM per transition, all plots aggregated (13 IPMs) 1 IPM per treatment combination, using mean parameter values across all transitions (4 IPMs) 1 IPM per historical treatment, using mean parameter values across all transitions and ignoring the current treatment (2 IPMs) 1 IPM per current treatment, using mean parameter values across all transitions and ignoring the historical treatment (2 IPMs) We will follow a slightly modified workflow from Chapter 4, since we are already familiar with the data and the basic structure of the models. In Chapter 6, we will begin by introducing the random effect of year to the basic models we constructed in Chapter 4 to build 13 new IPMs, one for each transition. Then, in Chapter 7, we will introduce the treatments to the basic model to build eight new IPMs and examine the effect of the different levels of the grazing treatments on the population growth rate. 5.2 Plotting by year x treatment Before we jump into the next models, let’s take a moment to explore the dataset a bit more in-depth than we have previously. We will start with the cleaned data, where we have updated the sizes of three individuals to no longer be smaller than the minimum size, since we already modified these data for the mean model. library ( ggplot2 ) library( patchwork ) grow &lt;- read.csv(&quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/growth_df_mod.csv&quot;) surv &lt;- read.csv(&quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/survival_df_mod.csv&quot;) recr &lt;- read.csv(&quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/recruitment_df_mod.csv&quot;) Bou_gra &lt;- read.csv(&quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/Bou_gra_clean.csv&quot;) When we created the mean model IPM, we aggregated all observations across year and treatment and created single plots for each vital rate. Let’s now examine plots showing the vital rate data for all year x treatment combinations, starting with survival. First we will set up a function to calculate the binned data at the scale of every year x treatment combination: df_binned_prop &lt;- function(ii, df_in, n_bins, siz_var, rsp_var, grid_y_l){ # make sub-selection of data df &lt;- subset( df_in, Year == grid_y_l$Year[ii] &amp; Treatment == grid_y_l$Treatment[ii] ) if( nrow( df ) == 0 ) return( NULL ) size_var &lt;- deparse( substitute( siz_var ) ) resp_var &lt;- deparse( substitute( rsp_var ) ) # binned survival probabilities h &lt;- ( max(df[,size_var], na.rm = T ) - min( df[,size_var], na.rm = T ) ) / n_bins lwr &lt;- min( df[,size_var], na.rm = T ) + ( h * c( 0:( n_bins - 1 ) ) ) upr &lt;- lwr + h mid &lt;- lwr + ( 1/2 * h ) binned_prop &lt;- function( lwr_x, upr_x, response ){ id &lt;- which( df[,size_var] &gt; lwr_x &amp; df[,size_var] &lt; upr_x ) tmp &lt;- df[id,] if( response == &#39;prob&#39; ){ return( sum( tmp[,resp_var], na.rm = T ) / nrow( tmp ) ) } if( response == &#39;n_size&#39; ){ return( nrow( tmp ) ) } } y_binned &lt;- Map( binned_prop, lwr, upr, &#39;prob&#39; ) %&gt;% unlist x_binned &lt;- mid y_n_size &lt;- Map( binned_prop, lwr, upr, &#39;n_size&#39; ) %&gt;% unlist # output data frame data.frame( xx = x_binned, yy = y_binned, nn = y_n_size ) %&gt;% setNames( c( size_var, resp_var, &#39;n_size&#39; ) ) %&gt;% mutate( Year = grid_y_l$Year[ii], Treatment = grid_y_l$Treatment[ii] ) } Now, we need to use expand.grid( ) to create a dataframe of all year x treatment combinations that we can use as the input to the function we created: grid_y_t &lt;- expand.grid( Year = surv$Year %&gt;% unique %&gt;% sort, Treatment = surv$Treatment %&gt;% unique %&gt;% sort, stringsAsFactors = F ) Then we use lapply( ) to apply our binning function across all treatment x year combinations: surv_bin_t &lt;- lapply( 1:nrow( grid_y_t ), df_binned_prop, Bou_gra, 10, logsize, survives_tplus1, grid_y_t ) And now we can create a big dataframe to plot the survival data in a large “panel plot”: surv_t_pan_df &lt;- bind_rows( surv_bin_t ) %&gt;% mutate( trt_lab = Treatment ) %&gt;% mutate( transition = paste( paste0( Year ), substr( paste0( Year + 1 ), 3, 4 ), sep = &#39;-&#39; ) ) %&gt;% mutate( year = as.integer( Year - 1996 ), treatment = Treatment %&gt;% as.factor %&gt;% as.integer ) ggplot( data = surv_t_pan_df, aes( x = logsize, y = survives_tplus1 ) ) + geom_point( alpha = 1, pch = 16, size = 0.7, color = &#39;red&#39;) + scale_y_continuous( breaks = c( 0.1, 0.5, 0.9 ) ) + # split in panels facet_grid( trt_lab ~ transition ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ), strip.text.y = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39;) ), strip.text.x = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39;) ), strip.switch.pad.wrap = unit( &#39;0.5&#39;, unit=&#39;mm&#39; ), panel.spacing = unit( &#39;0.5&#39;, unit=&#39;mm&#39; ) ) + labs( x = expression( &#39;log(size)&#39;[t0] ), y = expression( &#39;Survival to time t1&#39; ) ) At this scale, it’s kinda hard to tell what’s going on. The deficit of data we noticed in the first few years combined with the unequal amount of datapoints in each treatment makes the data fairly noisy in certain years. Even though the data indicate that there might be a significant year x treatment effect in the model, we do not have sufficient data to model this effect. Now we will look at the growth data. First we need to create the appropriate dataframe to make a “panel plot” like we did with the survival data: grow_t_pan_df &lt;- grow %&gt;% mutate( trt_lab = Treatment ) %&gt;% mutate( transition = paste( paste0( Year ), substr( paste0( Year + 1 ), 3, 4 ), sep = &#39;-&#39; ) ) %&gt;% mutate( year = as.integer( Year - 1996 ), treatment = Treatment %&gt;% as.factor %&gt;% as.integer ) Let’s take a look at the panel plot: ggplot(data = grow_t_pan_df, aes( x = logsize, y = log( size_tplus1 ) ) ) + geom_point( alpha = 1, pch = 16, size = 0.7, color = &#39;red&#39; ) + # split in panels facet_grid( trt_lab ~ transition ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ), strip.text.y = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.text.x = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.switch.pad.wrap = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ), panel.spacing = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ) ) + labs( x = expression( &#39;log( size )&#39;[t0] ), y = expression( &#39;log( size )&#39;[t1] ) ) As with the survival data, the deficit of data in the earlier years and the difference in the number of datapoints across treatments is fairly apparent in this plot. Any effect of year x treatment might simply be due to the different sample sizes, and may not actually reflect any biological processes. Now we should look at the recruitment data. We will plot the number of adults at time t0 on the x axis, and the number of recruits at time t1 on the y axis. First we need to create the dataframe for a “panel plot” like we did with the survival and growth data: recr$Treatment &lt;- substr( recr$Quad, 1, 4 ) indiv_yr_trt &lt;- surv %&gt;% group_by( Quad ) %&gt;% count( Year ) %&gt;% rename( n_adults = n ) %&gt;% mutate( Year = Year + 1 ) recr_trt &lt;- recr %&gt;% left_join( indiv_yr_trt ) %&gt;% drop_na ## Joining with `by = join_by(Quad, Year)` recr_t_pan_df &lt;- recr_trt %&gt;% mutate( trt_lab = Treatment ) %&gt;% mutate( transition = paste( paste0( Year ), substr( paste0( Year + 1 ), 3, 4 ), sep = &#39;-&#39; ) ) %&gt;% mutate( year = as.integer( Year - 1996 ), treatment = Treatment %&gt;% as.factor %&gt;% as.integer ) And now the panel plot (dropping that one outlier with over 600 individuals in a single plot so we can actually view the data): recr_t_pan_df %&gt;% filter( n_adults &lt; 600 ) %&gt;% filter( NRquad &lt; 600 ) %&gt;% ggplot( aes( x = n_adults, y = NRquad ) ) + geom_point( alpha = 1, pch = 16, size = 0.7, color = &#39;red&#39; ) + # split in panels facet_grid( trt_lab ~ transition ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ), strip.text.y = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.text.x = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.switch.pad.wrap = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ), panel.spacing = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ) ) + labs( x = &quot;Number of adults at time t0&quot;, y = &quot;Number of recruits at time t1&quot; ) As with the survival and growth data, there seems to be differences in the recruitment data across the different year x treatment combinations, but again, it is difficult to discern any prominent patterns. Considering these three plots, we can see how fitting models that consider both the effects of year and treatment might result in overconfident predictions, due to the considerable inconsistency in the number of observations across each treatment x year combination. However, if we had found noticeable differences in the patterns of any of the vital rate data across each treatment x year, this might provide an opportunity to continue exploring the data or consider other ways to model the vital rates. Taking a moment to plot every combination of treatments and years can be a valuable exercise in understanding the data you are working with, and hopefully this chapter can provide a framework of example code to produce similar graphs in your own projects. "],["adyr.html", "6 Adler Dataset: Extensions to the Mean IPM - Incorporating Year", " 6 Adler Dataset: Extensions to the Mean IPM - Incorporating Year In this chapter, we will introduce the effect of year into the vital rate models we fit in Chapter 4 for the mean IPM. We will build from Chapters 4 and 5 to continue exploring the data at the year-specific level, re-fit the vital rate models, and parameterize 13 new IPMs. We will then compare the lambda values of the year-specific IPMs to the lambda of the mean IPM. As discussed in Chapter 5, this chapter will roughly follow the same workflow as Chapter 4, but I will not spend time reiterating the same things I detailed in Chapter 4. Therefore, a familiarity with the contents of Chapter 3 is necessary to follow along with this chapter! We will start by jumping to step 3, to discuss how our vital rate models are impacted when we consider the effect of year. 6.0.1 3. Making the parameter “shopping list” One approach to incorporating the effect of year on our vital rates is to use a random effect of year in the vital rate models. Introducing the random effect of year to our vital rate models modifies how we notate the models, and greatly expands our parameter “shopping list,” but it does not change the basic structure of the model. The result of this approach is that we can add all 13 IPMs to the PADRINO database under a single IPM ID, utilizing the _yr suffix functionality. Another approach is to divide the dataset into 13 subsets, one for each year transition, and then fit vital rate models to each subset independently. In this approach, it is possible to fit models with different expressions for each year. When we add these IPMs to PADRINO, we would need a unique IPM ID for each year. There are benefits and drawbacks of both approaches, and which approach you choose will depend on the data you are working with. For very large datasets with many more than 100 observations per year, fitting unique models for each year might make more sense than simply incorporating the random effect of year into the mean model. Alternatively, fitting unique models for each year may not make any difference if the best fitting models for each year follow the same model formula. In theory, you should test out both approaches with any dataset which has sufficient data, and compare the fit of the resulting models to choose how to proceed. For the sake of simplicity, we will explore the first approach in this chapter. In Chapter 7, we will compare both approaches while incorporating the effect of treatment, as it is easier to keep track of the different models when there are fewer levels to model independently. Let’s go through each vital rate and update our shopping list accordingly for the models which consider the random effect of year in the model. 6.0.1.1 Survival model In mathematical notation, the subscript on the model terms has an added \\(_{yr}\\) component: \\[Logit(s_{yr}(z)) = \\beta_{s0,yr} + \\beta_{s1,yr} * z\\] The \\(_{yr}\\) subscript tells us that we will have unique values for the intercept and slope for each year, resulting in a different output each year (but the model expression stays the same). In R, the model formula becomes: glmer( survives_t1 ~ size_t0 + ( size_t0 | Year ), data = survival_df, family = binomial ) The output of this model will now have different estimates of the slope and intercept for each year. In ipmr, we similarly append the _yr suffix to our model terms: s_yr = 1 / ( 1 + exp( -( beta_s0_yr + beta_s1_yr * size_0 ) ) ) Now, instead of only needing to estimate two parameter values, we will need to extract 26 parameter values from our model, one slope and one intercept for each year. We can calculate these values by subtracting the random effects (from ranef()) from the fixed effects (from fixef()) for each year, or by simply using the coef() function. 6.0.1.2 Growth model Similarly to the survival model, we add \\(_{yr}\\) to some of the model terms: \\[G_{yr}(z&#39;, z) = f_{g}(z&#39;, \\mu_{g,yr}, \\sigma_{g})\\] \\[\\mu_{g,yr} = \\beta_{g0,yr} + \\beta_{g1,yr} * z\\] \\[ \\sigma_{g} = \\sqrt{( a * e^{( b * z )} )}\\] Note that the function \\(f_{g}\\) does not get the \\(_{yr}\\) subscript appended, as this is the probability density function describing the size of the individual at time t1, which is the same function for each year. \\(G_{yr}(z&#39;, z)\\) gets the \\(_{yr}\\) subscript to describe that the input to the \\(f_{g}\\) function is dependent on year. Additionally, the growth variance model will be based on the aggregated data; there are no year-specific values. In R, the model formulas become: lmer( size_t1 ~ size_t0 + ( size_t0 | Year ), data = growth_df ) nls( y ~ a * exp( b * x ), start = list( a = 1, b = 0 ) ) Note that the model describing the variance of the growth model has not changed. As with the survival model, we will be able to extract values for the slope and intercept of the growth model at each year. In ipmr, we similarly append the _yr suffix to the relevant model terms: g_yr = Norm( mu_g_yr, sigma_g ) mu_g_yr = beta_g0_yr + beta_g1_yr * size_0 sigma_g = sqrt( a * exp( b * size_0 ) ) As in the survival model, we will now need to extract 13 estimates for each beta parameter, one for each yearly transition. 6.0.1.3 Recruitment model With the recruitment model, we will also add the \\(_{yr}\\) subscript to some of model terms, in this case just the per-capita recruitment rate: \\[F_{yr}(z&#39;) = \\beta_{f,yr} * r_{d}(z&#39;)\\] \\[r_{d}(z&#39;) = f_{r_{d}}(z&#39;, \\mu_{r_{d}}, \\sigma_{r_{d}})\\] In R, we will write the model as: glmer.nb( NRquad ~ ( 1 | Year ), data = recr_df ) For now, we will actually ignore this model. The year-specific estimates corresponding to each \\(\\beta_{f,yr}\\) will be calculated from the raw data. We will also calculate the values for the mean and standard deviation of recruitment size from the aggregated raw data. In ipmr, we will again append the _yr suffix the model term referring to the year-specific per-capita recruitment rate, but not to the other parameters: fy_yr = beta_f_yr * r_d r_d = Norm( recr_sz, recr_sd ) We therefore have 13 values of the year-specific per-capita recruitment rate and two values describing the size of recruits that we will need to calculate. 6.0.1.4 Other parameters As with our mean model, we will need to define the minimum and maximum sizes over which to integrate, and the number of bins that ipmr will use to solve the integral. We will use 200 bins for all of our IPMs and the minimum and maximum sizes will be constant across all years. 6.0.2 5. Creating new folders in the R project Since we are building these new IPMs from the same dataset we used to parameterize the mean IPM for Bouteloua gracilis, we should keep all of our files contained within the same R project. We want to make sure we keep our files organized, however, so we will just create some new folders named “Bou_gra_yr” to differentiate the two workflows. The R project should look like this:  Adler_Colorado ↳  Adler_Colorado.Rproj  data ↳  Bou_gra  Bou_gra_yr  R ↳  Bou_gra  Bou_gra_yr  results ↳  Bou_gra  Bou_gra_yr 6.0.3 6. Data formatting In the “data/Bou_gra” subfolder, we already have formatted and cleaned data that we want to work with. We can either keep these files where they are and load them directly into the script from that subfolder, or we can copy and paste the files into the “data/Bou_gra_yr” subfolder. Either way, we should create our “notes” text document and indicate that we are using the already-formatted data from the previous workflow. grow &lt;- read.csv( &quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/growth_df_mod.csv&quot; ) surv &lt;- read.csv( &quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/survival_df_mod.csv&quot; ) recr &lt;- read.csv( &quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/recruitment_df_mod.csv&quot; ) Bou_gra &lt;- read.csv( &quot;https://raw.githubusercontent.com/aspen1030/RUPDemo/main/Bou_gra_clean.csv&quot; ) 6.0.4 7. Plotting the raw data by year Since there is no need to do any new formatting to the data, we can call our data plotting script “01_Plotting.R” as it is our first script in this workflow. In the previous workflow, we began by plotting size at t0 and size at t1 as histograms to examine the distributions of the size data. We should now do the same for each year. Consider a plot of size at t0 for the year 2000 and a plot of size at t1 for the year 2000. It might be tempting to think that we could just plot size at t0 for each year, because size at t1 for the year 2000 might be equivalent to size at t0 for the year 2001. However, this is incorrect; a plot of size at t1 for the year 2000 will have fewer individuals than both plots of size at t0 for the years 2000 and 2001, as some of the individuals will die after the 2000 census and new individuals will be recruited in 2001 which were not present in the 2000 census. Therefore, we should prepare histograms of both size at t0 and size at t1 for all years. We will go ahead and skip directly to plotting the log-transformed sizes. Bou_gra$logsize_t1 &lt;- log( Bou_gra$size_tplus1 ) Bou_gra_long &lt;- pivot_longer( Bou_gra, cols = c( logsize, logsize_t1 ), names_to = &quot;size&quot;, values_to = &quot;size_value&quot; ) Bou_gra_long$size &lt;- as.factor( Bou_gra_long$size ) Bou_gra_long$Year_fac &lt;- as.factor( Bou_gra_long$Year ) size_labs &lt;- c( &quot;at time t0&quot;, &quot;at time t1&quot; ) names(size_labs) &lt;- c( &quot;logsize&quot;, &quot;logsize_t1&quot; ) # png( &#39;results/Bou_gra_yr/histograms.png&#39;, width = 6, height = 13, units = &quot;in&quot;, res = 150 ) Bou_gra_long %&gt;% ggplot( aes( x = size_value ) ) + geom_histogram( binwidth = 1 ) + facet_grid( Year_fac ~ size, scales = &quot;free_y&quot;, labeller = labeller( size = size_labs ) ) + labs( x = &quot;log( size )&quot;, y = &quot;Frequency&quot; ) # dev.off( ) From this plot, we can see that the distributions of size shift from time t0 to time t1 in all years, particularly with regards to the smallest individuals. A large loss in these individuals from time t0 to time t1 without noticeable increases in the middle size classes indicates high seedling mortality in a given year, which is most pronounced in 1999-2007. In some years, we see a shift in the main distribution from smaller sizes in time t0 to slightly larger sizes in time t1, which we would expect, as this suggests that individuals are indeed growing. Overall, since there do seem to be differences in the patterns between years, so modeling the data with the random effect of year seems like a reasonable choice. Additionally, the lower and upper bounds of size seem quite consistent across all years, so keeping the limits of integration constant across all years is also a reasonable choice. Let’s now plot the survival data by year. We must first setup a function which bins the survival data by size for each year, so that we can visualize the data as probabilities of survival for a given size class. We then apply the function across 13 years to create a list that we can plot faceted by year. df_binned_prop_year &lt;- function( ii, df_in, n_bins, siz_var, rsp_var, years ){ # make sub-selection of data df &lt;- subset( df_in, Year == years$Year[ii] ) if( nrow( df ) == 0 ) return( NULL ) size_var &lt;- deparse( substitute( siz_var ) ) resp_var &lt;- deparse( substitute( rsp_var ) ) # binned survival probabilities h &lt;- ( max(df[,size_var], na.rm = T ) - min( df[,size_var], na.rm = T ) ) / n_bins lwr &lt;- min( df[,size_var], na.rm = T ) + ( h * c( 0:( n_bins - 1 ) ) ) upr &lt;- lwr + h mid &lt;- lwr + ( 1/2 * h ) binned_prop &lt;- function( lwr_x, upr_x, response ){ id &lt;- which( df[,size_var] &gt; lwr_x &amp; df[,size_var] &lt; upr_x ) tmp &lt;- df[id,] if( response == &#39;prob&#39; ){ return( sum( tmp[,resp_var], na.rm = T ) / nrow( tmp ) ) } if( response == &#39;n_size&#39; ){ return( nrow( tmp ) ) } } y_binned &lt;- Map( binned_prop, lwr, upr, &#39;prob&#39; ) %&gt;% unlist x_binned &lt;- mid y_n_size &lt;- Map( binned_prop, lwr, upr, &#39;n_size&#39; ) %&gt;% unlist # output data frame data.frame( xx = x_binned, yy = y_binned, nn = y_n_size ) %&gt;% setNames( c( size_var, resp_var, &#39;n_size&#39; ) ) %&gt;% mutate( Year = years$Year[ii] ) } surv_yrs &lt;- data.frame( Year = surv$Year %&gt;% unique %&gt;% sort ) surv_bin_yrs &lt;- lapply( 1:13, df_binned_prop_year, Bou_gra, 20, logsize, survives_tplus1, surv_yrs ) surv_yr_pan_df &lt;- bind_rows( surv_bin_yrs ) %&gt;% mutate( transition = paste( paste0( Year ), substr( paste0( Year + 1 ), 3, 4 ), sep = &#39;-&#39; ) ) %&gt;% mutate( year = as.integer( Year - 1996 ) ) # png( &#39;results/Bou_gra_yr/survival_binned_yr.png&#39;, width = 10, height = 6, units = &quot;in&quot;, res = 150 ) ggplot( data = surv_yr_pan_df, aes( x = logsize, y = survives_tplus1 ) ) + geom_point( alpha = 0.5, pch = 16, size = 1, color = &#39;red&#39; ) + scale_y_continuous( breaks = c( 0.1, 0.5, 0.9 ) ) + # split in panels facet_wrap( .~ transition, nrow = 4 ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ), strip.text.y = element_text( size = 5, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.text.x = element_text( size = 5, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.switch.pad.wrap = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ), panel.spacing = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ) ) + labs( x = expression( &#39;log(size)&#39;[t0] ), y = expression( &#39;Survival to time t1&#39; ) ) # dev.off( ) As always happens in demographic data, the relationship between survival probability at time t1 and size a time t0 is variable through years, further justifying our choice to create an IPM for each year. Next, we should plot the growth data faceted by year. grow_yr_pan_df &lt;- grow %&gt;% mutate( transition = paste( paste0( Year ), substr( paste0( Year + 1 ), 3, 4 ), sep = &#39;-&#39; ) ) %&gt;% mutate( year = as.integer( Year - 1996 ) ) # png( &#39;results/Bou_gra_yr/growth_yr.png&#39;, width = 10, height = 6, units = &quot;in&quot;, res = 150 ) ggplot(data = grow_yr_pan_df, aes( x = logsize, y = log( size_tplus1 ) ) ) + geom_point( alpha = 0.5, pch = 16, size = 0.7, color = &#39;red&#39; ) + # split in panels facet_wrap( .~ transition, nrow = 4 ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ), strip.text.y = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.text.x = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.switch.pad.wrap = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ), panel.spacing = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ) ) + labs( x = expression( &#39;log( size )&#39;[t0] ), y = expression( &#39;log( size )&#39;[t1] ) ) # dev.off( ) Here, we see the effect of transition year on the growth vital rate. Some transition years show less spread in the data (03-04) compared to others (01-02), and we can start to see how the different years might be modeled with different slopes and/or intercepts. Now we can plot the recruitment data faceted by year. For visualiztion purposes, I’ve filtered out the largest value (the single year where over 600 recruits were added to the dataset). indiv_qd &lt;- surv %&gt;% group_by( Quad ) %&gt;% count( Year ) %&gt;% rename( n_adults = n ) %&gt;% mutate( Year = Year + 1 ) repr_yr &lt;- indiv_qd %&gt;% left_join( recr ) %&gt;% mutate( repr_pc = NRquad / n_adults ) %&gt;% mutate( Year = Year - 1 ) %&gt;% drop_na ## Joining with `by = join_by(Quad, Year)` # png( &#39;results/Bou_gra_yr/recruit_yr.png&#39;, width = 10, height = 6, units = &quot;in&quot;, res = 150 ) repr_yr %&gt;% filter( NRquad != max( repr_yr$NRquad ) ) %&gt;% filter( n_adults != max( repr_yr$n_adults ) ) %&gt;% ggplot( aes( x = n_adults, y = NRquad ) ) + geom_point( alpha = 1, pch = 16, size = 1, color = &#39;red&#39; ) + facet_wrap( .~ Year, nrow = 4 ) + theme_bw( ) + theme( axis.text = element_text( size = 8 ), title = element_text( size = 10 ), strip.text.y = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.text.x = element_text( size = 8, margin = margin( 0.5, 0.5, 0.5, 0.5, &#39;mm&#39; ) ), strip.switch.pad.wrap = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ), panel.spacing = unit( &#39;0.5&#39;, unit = &#39;mm&#39; ) ) + labs( x = expression( &#39;Number of adults &#39;[ t0] ), y = expression( &#39;Number of recruits &#39;[ t1] ) ) # dev.off( ) As with the other vital rates, we see some differences between the years which we will incorporate into our IPM by taking a year-specific per-capita mean recruitment rate across all plots. We should also take a moment to plot histograms of the recruit size data, so we can determine if we should take year-specific means and standard deviations of recruit size, or if we expect that taking the global mean and standard deviation would be sufficient. recSize &lt;- Bou_gra %&gt;% subset( recruit == 1) recSize$Year_fac &lt;- as.factor( recSize$Year ) # png( &#39;results/Bou_gra_yr/recr_histograms.png&#39;, width = 10, height = 6, units = &quot;in&quot;, res = 150 ) recSize %&gt;% ggplot( aes( x = logsize ) ) + geom_histogram( ) + facet_wrap( Year_fac ~ ., scales = &quot;free_y&quot;, nrow = 4 ) + labs( x = expression(&#39;log( size )&#39;[t0]), y = &quot;Frequency&quot; ) # dev.off( ) Note that I’ve used the \"free_y\" option in facet_wrap() to allow the y-axis to be fitted to each panel individually, while the x-axis remains fixed across all panels. This ensures that we can actually compare the distributions across years, even though 2002 has an order of magnitude more recruits than any other year. While there is some variability in the distributions across the years, the majority of recruits enter the population at the minimum size and there doesn’t seem to be any particularly wild years. Keep in mind also that the sizes on the x-axis are log-transformed; we’re dealing with incredibly small individuals overall. While the differences may seem pronounced on this scale, it is important to keep in mind the biological scale of these individuals. An individual with log(size) around -10 has an area of 0.005 cm\\(^2\\), whereas an individual with log(size) of -6 has an area of 0.25 cm\\(^2\\). These are both very, very small plants; differences in size at this scale may very well be attributable to errors in how the plants were recorded on the paper maps or digitized. We will therefore keep the recruitment size parameters constant throughout all years. 6.0.5 8. Fitting vital rate models with the random effect of year Since we already wrote our model formulas in step 3, we are ready to jump right into fitting these models to the data. 6.0.5.1 Survival model Here is the model we wrote previously: surv_df &lt;- surv %&gt;% mutate( logarea = log( basalArea_genet ) ) su_mod_yr &lt;- glmer( survives_tplus1 ~ logarea + ( logarea | Year ), data = surv_df, family = binomial ) Let’s plot the model over the data and check the model fit: ranef_su &lt;- data.frame( coef( su_mod_yr )[1] ) years_v &lt;- c( 1997:2009 ) surv_yr_plots &lt;- function( i ){ surv_temp &lt;- as.data.frame( surv_bin_yrs[[i]] ) x_temp &lt;- seq( min( surv_temp$logsize, na.rm = T ), max( surv_temp$logsize, na.rm = T ), length.out = 100) pred_temp &lt;- boot::inv.logit( ranef_su[i,1] + ranef_su[i,2] * x_temp ) pred_temp_df &lt;- data.frame( logarea = x_temp, survives_tplus1 = pred_temp ) temp_plot &lt;- surv_temp %&gt;% ggplot( ) + geom_point( aes( x = logsize, y = survives_tplus1 ) ) + geom_line( data = pred_temp_df, aes( x = logarea, y = survives_tplus1 ), color = &#39;red&#39;, lwd = 1 ) + labs( title = paste0( years_v[i] ), x = expression( &#39;log( size )&#39;[t0] ), y = expression( &#39;Survival probability &#39;[ t1] ) ) if( i %in% c( 2:4, 6:8, 10:12 ) ){ temp_plot &lt;- temp_plot + theme( axis.title.y = element_blank( ) ) } return(temp_plot) } surv_yrs &lt;- lapply( 1:13, surv_yr_plots ) surv_years &lt;- wrap_plots( surv_yrs ) + plot_layout( nrow = 4 ) # png( &#39;results/Bou_gra_yr/survival_pred.png&#39;, width = 10, height = 8, units = &quot;in&quot;, res = 150 ) surv_years # dev.off( ) These plots show the year-specific models plotted over the binned data. Some years seem to be fit better than others, but overall the model describes the data fairly well. 6.0.5.2 Growth model Here is the first model that we wrote previously, describing the growth of individuals: grow_df &lt;- grow %&gt;% mutate( logarea_t0 = log( basalArea_genet ), logarea_t1 = log( size_tplus1 ) ) gr_mod_yr &lt;- lmer( logarea_t1 ~ logarea_t0 + ( logarea_t0 | Year ), data = grow_df ) Let’s plot the model over the data and check the model fit: ranef_gr &lt;- data.frame( coef( gr_mod_yr )[1] ) grow_yr_plots &lt;- function( i ){ temp_plot &lt;- grow_df %&gt;% filter( Year == i ) %&gt;% ggplot( ) + geom_point( aes( x = logarea_t0, y = logarea_t1 ) ) + geom_abline( aes( intercept = ranef_gr[which(rownames( ranef_gr ) == i ),1], slope = ranef_gr[which(rownames( ranef_gr ) == i ),2] ), color = &quot;red&quot;, lwd = 1 ) + labs( title = paste0( i ), x = expression( &#39;log( size ) &#39;[ t0] ), y = expression( &#39;log( size ) &#39;[ t1] ) ) if( i %in% c( 1998:2000, 2002:2004, 2006:2008 ) ){ temp_plot &lt;- temp_plot + theme( axis.title.y = element_blank( ) ) } return(temp_plot) } grow_yrs &lt;- lapply( 1997:2009, grow_yr_plots ) grow_years &lt;- wrap_plots( grow_yrs ) + plot_layout( nrow = 4 ) # png( &#39;results/Bou_gra_yr/grow_pred.png&#39;, width = 10, height = 6, units = &quot;in&quot;, res = 150 ) grow_years # dev.off( ) From these plots, it is fairly difficult to see how each plot has a unique model intercept and slope, since each year seems to be fit pretty well. In general, most year-specific models are underestimating the size at time t1 of the largest individuals, but the regression line seems pretty great otherwise. Before we move onto the growth variance model, let’s take a moment to consider if this really is the best model. With both the survival and growth models, we assumed that the simplest model with only linear terms best describes the data. With the survival model, I will just reveal that including nonlinear terms causes the model to fail to converge. It’s hard to visually confirm that our model fits well without linear terms due to the more abstract nature of the logistic regression, but with our linear growth model, we can see in the previous plot that there might be room for improvement. Let’s compare this growth model to two other models which include nonlinear terms. The first will include a quadratic term, and the second will include a quadratic term and a cubic term. The expressions for these models are: \\[\\mu_{g2,yr} = \\beta_{g0,yr} + \\beta_{g1,yr} * z + \\beta_{g2,yr} * z^2\\] \\[\\mu_{g3,yr} = \\beta_{g0,yr} + \\beta_{g1,yr} * z + \\beta_{g2,yr} * z^2 + \\beta_{g3,yr} * z^3\\] grow_df$logarea_t0_2 &lt;- grow_df$logarea_t0^2 grow_df$logarea_t0_3 &lt;- grow_df$logarea_t0^3 gr_mod_yr2 &lt;- lmer( logarea_t1 ~ logarea_t0 + logarea_t0_2 + ( logarea_t0 | Year ), data = grow_df ) gr_mod_yr3 &lt;- lmer( logarea_t1 ~ logarea_t0 + logarea_t0_2 + logarea_t0_3 + ( logarea_t0 | Year ), data = grow_df ) g_mods &lt;- c( gr_mod_yr, gr_mod_yr2, gr_mod_yr3 ) AICtab( g_mods, weights = T ) ## dAIC df weight ## model2 0.0 7 0.9911 ## model3 9.4 8 0.0089 ## model1 460.1 6 &lt;0.001 Comparing the AIC values of these models indicates that including the quadratic term greatly improves the fit of the model over the model only including the linear term, and adding the cubic term does not improve the fit of the model with the quadratic term. Let’s plot the model with the quadratic term: ranef_gr2 &lt;- data.frame( coef( gr_mod_yr2 )[1] ) grow_yr_plots2 &lt;- function( i ){ temp_f &lt;- function( x ) ranef_gr2[which(rownames( ranef_gr2 ) == i ),1] + ranef_gr2[which(rownames( ranef_gr2 ) == i ),2] * x + ranef_gr2[which(rownames( ranef_gr2 ) == i ),3] * x^2 temp_plot &lt;- grow_df %&gt;% filter( Year == i ) %&gt;% ggplot( ) + geom_point( aes( x = logarea_t0, y = logarea_t1 ) ) + geom_function( fun = temp_f, color = &quot;green&quot;, lwd = 1 ) + labs( title = paste0( i ), x = expression( &#39;log( size ) &#39;[ t0] ), y = expression( &#39;log( size ) &#39;[ t1] ) ) if( i %in% c( 1998:2000, 2002:2004, 2006:2008 ) ){ temp_plot &lt;- temp_plot + theme( axis.title.y = element_blank( ) ) } return(temp_plot) } grow_yrs2 &lt;- lapply( 1997:2009, grow_yr_plots2 ) grow_years2 &lt;- wrap_plots( grow_yrs2 ) + plot_layout( nrow = 4 ) # png( &#39;results/Bou_gra_yr/grow_pred_2.png&#39;, width = 10, height = 6, units = &quot;in&quot;, res = 150 ) grow_years2 # dev.off( ) And just for fun, I’ll also plot the model with the cubic term and we can compare all three models: Here, I’ve plotted the model with only the linear term on the top row (red line), the model with the added quadratic term on the middle row (green line), and the model with the added cubic term on the bottom row (blue line). I have also only plotted 2003 through 2009 for improved readability. First, notice how the green line fits the largest and smallest individuals much better than the red. Second, notice that there is virtually no change when comparing the fit of the green vs the blue line. Therefore, choosing the model with the quadratic term but without the cubic term makes the most sense here. Now that we’ve decided on which growth model we will choose, we need to fit the growth variance model. x &lt;- fitted( gr_mod_yr2 ) y &lt;- resid( gr_mod_yr2 )^2 gr_var &lt;- nls( y ~ a * exp( b * x ), start = list( a = 1, b = 0 ) ) This is the same model as the one we wrote in Chapter 4.4.8.1, as the growth variance model does not change by year. 6.0.5.3 Recruitment model First, we model the number of recruits using the negative binomial distribution with the random effect of year: rec_mod &lt;- glmer.nb( NRquad ~ ( 1 | Year ), data = recr ) Then, we use the predict( ) function to predict the number of recruits per year per quad, group the data by year, and sum up the observed and predicted number of recruits per year across all quads: recr_df &lt;- recr %&gt;% mutate( pred_mod = predict( rec_mod, type = &#39;response&#39; ) ) rec_sums_df &lt;- recr_df %&gt;% group_by( Year ) %&gt;% summarise( NRquad = sum( NRquad ), pred_mod = sum( pred_mod ) ) %&gt;% ungroup Then, we sum up the number of adults present in each year, and rename the year column so we can link the number of adults at time t0 to the number of recruits at time t1: indiv_yr &lt;- surv_df %&gt;% count( Year ) %&gt;% rename( n_adults = n ) %&gt;% mutate( Year = Year + 1 ) Finally, we join these two dataframes and calculate a per-capita recruitment rate based on our model and on the observed recruits: repr_pc_yr &lt;- indiv_yr %&gt;% left_join( rec_sums_df ) %&gt;% mutate( repr_percapita = pred_mod / n_adults ) %&gt;% mutate( repr_pc_obs = NRquad / n_adults ) %&gt;% mutate( Year = Year - 1 ) %&gt;% drop_na And now we can plot the data against the model predictions: # png( &#39;results/Bou_gra_yr/recruit_pred.png&#39;, width = 6, height = 4, units = &quot;in&quot;, res = 150 ) repr_pc_yr %&gt;% ggplot() + geom_point( aes( x = repr_pc_obs, y = repr_percapita ) ) + geom_abline( aes( intercept = 0, slope = 1 ), color = &quot;red&quot;, lwd = 2, alpha = 0.5 ) + labs( x = &quot;Observed per capita recruitment&quot;, y = &quot;Predicted per capita recruitment&quot; ) # dev.off( ) On the x axis, we have the observed per-capita recruitment rates and on the y axis are the corresponding modeled per-capita recruitment rates. Each point shows a single transition year. Most points are very close to the 1:1 line, indicating the model predicts most years well. We see that the two points with the highest observed recruitment are less accurately predicted by the model, but this is not so unrealistic: recall that one plot with over 600 recruits in a single year, which is clearly going to have a strong influence on the mean recruitment rate of that year. 6.0.5.4 Exporting parameter estimates Now that we’ve modeled all of the vital rates and calculated all of the varying parameters, we need to store the values in dataframes that we can work with in the next step, constructing the IPMs. As with our mean model, we will export one dataframe for each model. We will also rearrange the parameters into two dataframes: one which contains mean model parameter estimates plus all parameters which do not vary across the years, and one which contains only estimates of the parameters which vary across the years. Let’s start with the survival model: su_yr_r &lt;- data.frame( coefficient = paste0( &quot;year_&quot;, rownames( coef( su_mod_yr )$Year ) ), value = coef( su_mod_yr )$Year[,&quot;(Intercept)&quot;] ) su_la_r &lt;- data.frame( coefficient = paste0( &quot;logarea_&quot;, rownames( coef( su_mod_yr )$Year ) ), value = coef( su_mod_yr )$Year[,&quot;logarea&quot;] ) surv_out_yr &lt;- Reduce( function(...) rbind(...), list( su_la_r, su_yr_r ) ) %&gt;% mutate( coefficient = as.character( coefficient ) ) # write.csv( surv_out_yr, &quot;data/Bou_gra_yr/surv_pars.csv&quot;, row.names = F ) And then the growth model: var_fe &lt;- data.frame( coefficient = names( coef( gr_var ) ), value = coef( gr_var ) ) year_re &lt;- data.frame( coefficient = paste0( &quot;year_&quot;, rownames( coef( gr_mod_yr2 )$Year ) ), value = coef( gr_mod_yr2 )$Year[,&quot;(Intercept)&quot;] ) la_re &lt;- data.frame( coefficient = paste0( &quot;logarea_t0_&quot;, rownames( coef( gr_mod_yr2 )$Year ) ), value = coef( gr_mod_yr2 )$Year[,&quot;logarea_t0&quot;] ) la2_re &lt;- data.frame( coefficient = paste0( &quot;logarea_t0_2_&quot;, rownames( coef( gr_mod_yr2 )$Year ) ), value = coef( gr_mod_yr2 )$Year[,&quot;logarea_t0_2&quot;] ) grow_out_yr &lt;- Reduce( function(...) rbind(...), list( var_fe, la_re, la2_re, year_re ) ) %&gt;% mutate( coefficient = as.character( coefficient ) ) # write.csv( grow_out_yr, &quot;data/Bou_gra_yr/grow_pars.csv&quot;, row.names = F ) And the recruitment model: rc_pc &lt;- data.frame( coefficient = paste0( &quot;rec_pc_&quot;, repr_pc_yr$Year ), value = repr_pc_yr$repr_percapita ) rc_sz &lt;- data.frame( coefficient = c( &quot;rec_siz&quot;, &quot;rec_sd&quot; ), value = c( mean( recSize$logsize ), sd( recSize$logsize ) ) ) recr_out_yr &lt;- Reduce( function(...) rbind(...), list( rc_pc, rc_sz ) ) %&gt;% mutate( coefficient = as.character( coefficient ) ) # write.csv( recr_out_yr, &quot;data/Bou_gra_yr/recr_pars.csv&quot;, row.names = F ) Next, let’s put the constant parameters, fixed effects estimates, and mean parameter estimates into a dataframe. These values can be used to construct the mean IPM, or to construct IPMs holding individual vital rate parameters constant for testing. This dataframe also contains the parameters which will not vary across all year-specific IPMs, such as the bounds of integration, the number of bins, the parameters for the growth variance model, and the recruit size parameters. constants &lt;- data.frame( coefficient = c( &quot;recr_sz&quot;, &quot;recr_sd&quot;, &quot;a&quot;, &quot;b&quot;, &quot;L&quot;, &quot;U&quot;, &quot;mat_siz&quot; ), value = c( mean( recSize$logsize ), sd( recSize$logsize ), as.numeric(coef(gr_var)[1]), as.numeric(coef(gr_var)[2]), grow_df$logarea_t0 %&gt;% min, grow_df$logarea_t0 %&gt;% max, 200 ) ) surv_fe &lt;- data.frame( coefficient = c( &quot;surv_b0&quot;, &quot;surv_b1&quot; ), value = fixef( su_mod_yr ) ) grow_fe &lt;- data.frame( coefficient = c( &quot;grow_b0&quot;, &quot;grow_b1&quot;, &quot;grow_b2&quot; ), value = fixef( gr_mod_yr2 ) ) rec_fe &lt;- data.frame( coefficient = &quot;fecu_b0&quot;, value = mean( repr_pc_yr$repr_percapita ) ) pars_cons &lt;- Reduce(function(...) rbind(...), list( surv_fe, grow_fe, rec_fe, constants ) ) %&gt;% mutate( coefficient = as.character( coefficient ) ) rownames( pars_cons ) &lt;- 1:13 pars_cons_wide &lt;- as.list( pivot_wider( pars_cons, names_from = &quot;coefficient&quot;, values_from = &quot;value&quot; ) ) # write.csv( pars_cons_wide, &quot;data/Bou_gra_yr/pars_cons.csv&quot;, row.names = F ) Lastly, let’s put all of the varying parameters into their own dataframe, too. This will also be useful in the testing stage. su_b0 &lt;- data.frame( coefficient = paste0( &quot;surv_b0_&quot;, rownames( coef( su_mod_yr )$Year ) ), value = coef( su_mod_yr )$Year[,&quot;(Intercept)&quot;] ) su_b1 &lt;- data.frame( coefficient = paste0( &quot;surv_b1_&quot;, rownames( coef( su_mod_yr )$Year ) ), value = coef( su_mod_yr )$Year[,&quot;logarea&quot;] ) grow_b0 &lt;- data.frame( coefficient = paste0( &quot;grow_b0_&quot;, rownames( coef( gr_mod_yr2 )$Year ) ), value = coef( gr_mod_yr2 )$Year[,&quot;(Intercept)&quot;] ) grow_b1 &lt;- data.frame( coefficient = paste0( &quot;grow_b1_&quot;, rownames( coef( gr_mod_yr2 )$Year ) ), value = coef( gr_mod_yr2 )$Year[,&quot;logarea_t0&quot;] ) grow_b2 &lt;- data.frame( coefficient = paste0( &quot;grow_b2_&quot;, rownames( coef( gr_mod_yr2 )$Year ) ), value = coef( gr_mod_yr2 )$Year[,&quot;logarea_t0_2&quot;] ) fecu_b0 &lt;- data.frame( coefficient = paste0( &quot;fecu_b0_&quot;, repr_pc_yr$Year ), value = repr_pc_yr$repr_percapita ) pars_var &lt;- Reduce(function(...) rbind(...), list( su_b0, su_b1, grow_b0, grow_b1, grow_b2, fecu_b0 ) ) pars_var_wide &lt;- as.list( pivot_wider( pars_var, names_from = &quot;coefficient&quot;, values_from = &quot;value&quot; ) ) # write.csv( pars_var_wide, &quot;data/Bou_gra_yr/pars_var.csv&quot;, row.names = F ) Now that all of our parameter values are organized, we are ready to build the year-specific IPM! 6.0.6 9. Building the year-specific IPMs from scratch In Chapter 4, we built the mean IPM from scratch, which we will now expand upon to create the year-specific IPMs. We can borrow most of the vital rate functions we wrote in Chapter 4, even though our vital rate expressions now contain the \\(_{yr}\\) subscript on most of the terms. Once we build the IPM, we can just input the year-specific parameter values into the kernel function one year at a time. We need to first update the growth vital rate function to accommodate the changes we made when we selected a better-fitting model. Recall the mathematical notation of the new model: \\[\\mu_{g2,yr} = \\beta_{g0,yr} + \\beta_{g1,yr} * z + \\beta_{g2,yr} * z^2\\] And how we wrote the model expression in R: lmer( logarea_t1 ~ logarea_t0 + logarea_t0_2 + ( logarea_t0 | Year ), data = grow_df ) Let’s write the function describing the model of standard deviation of the growth model, and then the revised function describing growth from size at time t0 to size at time t1. The revised growth function just has the added quadratic term: # Standard deviation of growth model grow_sd &lt;- function( x, pars ) { pars$a * ( exp( pars$b * x ) ) %&gt;% sqrt } # Growth from size x to size y gxy &lt;- function( x, y, pars ) { return( dnorm( y, mean = pars$grow_b0 + pars$grow_b1*x + pars$grow_b2*x^2, sd = grow_sd( x, pars ) ) ) } Now we can load the survival functions, and the function describing the P kernel: # Inverse logit inv_logit &lt;- function( x ) { exp( x ) / ( 1 + exp( x ) ) } # Survival of x-sized individual to time t1 sx &lt;- function( x, pars ) { return( inv_logit( pars$surv_b0 + pars$surv_b1 * x ) ) } # Transition of x-sized individual to y-sized individual at time t1 pxy &lt;- function( x, y, pars ) { return( sx( x, pars ) * gxy( x, y, pars ) ) } And finally the recruitment function: # Per-capita production of y-sized recruits fy &lt;- function( y, pars, h ){ n_recr &lt;- pars$fecu_b0 recr_y &lt;- dnorm( y, pars$recr_sz, pars$recr_sd ) * h recr_y &lt;- recr_y / sum( recr_y ) f &lt;- n_recr * recr_y return( f ) } Next we can assemble the kernel function. I have removed the comments to save space; refer to Chapter 4.4.9 for further explanation. kernel &lt;- function( pars ) { n &lt;- pars$mat_siz L &lt;- pars$L U &lt;- pars$U h &lt;- ( U - L ) / n b &lt;- L + c( 0:n ) * h y &lt;- 0.5 * ( b[1:n] + b[2:( n + 1 )] ) Fmat &lt;- matrix( 0, n, n ) Fmat[] &lt;- matrix( fy( y, pars, h ), n, n ) Smat &lt;- c( ) Smat &lt;- sx( y, pars ) Gmat &lt;- matrix( 0, n, n ) Gmat[] &lt;- t( outer( y, y, gxy, pars ) ) * h Tmat &lt;- matrix( 0, n, n ) for( i in 1:( n / 2 ) ) { Gmat[1,i] &lt;- Gmat[1,i] + 1 - sum( Gmat[,i] ) Tmat[,i] &lt;- Gmat[,i] * Smat[i] } for( i in ( n / 2 + 1 ):n ) { Gmat[n,i] &lt;- Gmat[n,i] + 1 - sum( Gmat[,i] ) Tmat[,i] &lt;- Gmat[,i] * Smat[i] } k_yx &lt;- Fmat + Tmat return( list( k_yx = k_yx, Fmat = Fmat, Tmat = Tmat, Gmat = Gmat, meshpts = y ) ) } Using the constant parameters, mean values of calculated parameters, and estimates of the fixed effects in the GLMMs, we can then calculate the mean population growth rate, \\(\\lambda_{mean}\\): pars_mean &lt;- pars_cons_wide lambda_ipm &lt;- function( i ) { return( Re( eigen( kernel( i )$k_yx )$value[1] ) ) } lam_mean &lt;- lambda_ipm( pars_mean ) lam_mean ## [1] 1.221076 Now, we can input the year-specific parameter values and calculate the population growth rates for each year, \\(\\lambda_{yr}\\): bogr_yr &lt;- c( 1997:2009 ) pars_yr &lt;- vector( mode = &quot;list&quot;, length = length( bogr_yr ) ) extr_value_list &lt;- function( x, field ) { return( as.numeric( x[paste0( field )] %&gt;% unlist( ) ) ) } prep_pars &lt;- function( i ) { yr_now &lt;- bogr_yr[i] pars_year &lt;- list( surv_b0 = extr_value_list( pars_var_wide, paste( &quot;surv_b0&quot;, yr_now, sep = &quot;_&quot; ) ), surv_b1 = extr_value_list( pars_var_wide, paste( &quot;surv_b1&quot;, yr_now, sep = &quot;_&quot; ) ), grow_b0 = extr_value_list( pars_var_wide, paste( &quot;grow_b0&quot;, yr_now, sep = &quot;_&quot; ) ), grow_b1 = extr_value_list( pars_var_wide, paste( &quot;grow_b1&quot;, yr_now, sep = &quot;_&quot; ) ), grow_b2 = extr_value_list( pars_var_wide, paste( &quot;grow_b2&quot;, yr_now, sep = &quot;_&quot; ) ), a = extr_value_list( pars_cons_wide, &quot;a&quot; ), b = extr_value_list( pars_cons_wide, &quot;b&quot; ), fecu_b0 = extr_value_list( pars_var_wide, paste( &quot;fecu_b0&quot;, yr_now, sep = &quot;_&quot; ) ), recr_sz = extr_value_list( pars_cons_wide, &quot;recr_sz&quot; ), recr_sd = extr_value_list( pars_cons_wide, &quot;recr_sd&quot; ), L = extr_value_list( pars_cons_wide, &quot;L&quot; ), U = extr_value_list( pars_cons_wide, &quot;U&quot; ), mat_siz = 200 ) return( pars_year ) } pars_yr &lt;- lapply( 1:length( bogr_yr ), prep_pars ) calc_lambda &lt;- function( i ) { lam &lt;- Re( eigen( kernel( pars_yr[[i]] )$k_yx )$value[1] ) return( lam ) } lambdas_yr &lt;- lapply( 1:length( bogr_yr ), calc_lambda ) names( lambdas_yr ) &lt;- bogr_yr 6.0.6.1 Comparing the year-specific lambdas We will examine these values in a moment. We should also calculate the mean kernel from all of the year-specific kernels, and calculate some other mean lambda values to compare to the lambda from the mean model. year_kern &lt;- function( i ) { return( kernel( pars_yr[[i]] )$k_yx ) } kern_yr &lt;- lapply( 1:length( bogr_yr ), year_kern ) all_mat &lt;- array( dim = c( 200, 200, 13 ) ) for( i in 1:length( bogr_yr ) ) { all_mat[,,i] &lt;- as.matrix( kern_yr[[i]] ) } mean_kern &lt;- apply( all_mat, c( 1, 2 ), mean ) lam_mean_kern &lt;- Re( eigen( mean_kern )$value[1] ) lam_mean_kern ## [1] 1.219977 Now, let’s check the observed population growth rate (changes in number of individuals) against the projected population growth rate. The population growth rate should be more precise than asymptotic lambda in checking our models. Asymptotic lambda is the population growth rate at the “stable stage distribution” - the stage distribution that the population reaches after a long period of time. # Population counts at time t0 pop_counts_t0 &lt;- Bou_gra %&gt;% group_by( Year, Quad ) %&gt;% summarize( n_t0 = n( ) ) %&gt;% ungroup %&gt;% mutate( Year = Year + 1 ) # Population counts at time t1 pop_counts_t1 &lt;- Bou_gra %&gt;% group_by( Year, Quad ) %&gt;% summarize( n_t1 = n( ) ) %&gt;% ungroup # Calculate observed population growth rates, # accounting for discontinued sampling! pop_counts &lt;- left_join( pop_counts_t0, pop_counts_t1 ) %&gt;% # by dropping NAs, we remove gaps in sampling! drop_na %&gt;% group_by( Year ) %&gt;% summarise( n_t0 = sum( n_t0 ), n_t1 = sum( n_t1 ) ) %&gt;% ungroup %&gt;% mutate( obs_pgr = n_t1 / n_t0 ) %&gt;% mutate( lambda = lambdas_yr %&gt;% unlist ) lam_mean_yr &lt;- mean( pop_counts$lambda, na.rm = T ) lam_mean_count &lt;- mean( pop_counts$obs_pgr, na.rm = T ) lam_mean_geom &lt;- exp( mean( log( pop_counts$obs_pgr ), na.rm = T ) ) lam_mean_geom ## [1] 1.164344 lam_mean_overall &lt;- sum( pop_counts$n_t1 ) / sum( pop_counts$n_t0 ) lam_mean_overall ## [1] 1.05844 We can also check our model by projecting a population vector for each year using the year-specific models, and compare the projected population to the observed population: count_indivs_by_size &lt;- function( size_vector, lower_size, upper_size, matrix_size ){ size_vector %&gt;% cut( breaks = seq( lower_size - 0.00001, upper_size + 0.00001, length.out = matrix_size + 1 ) ) %&gt;% table %&gt;% as.vector } yr_pop_vec &lt;- function( i ) { vec_temp &lt;- surv_df %&gt;% filter( Year == i ) %&gt;% select( logsize ) %&gt;% unlist( ) min_sz &lt;- pars_mean$L max_sz &lt;- pars_mean$U pop_vec &lt;- count_indivs_by_size( vec_temp, min_sz, max_sz, 200 ) return( pop_vec ) } year_pop &lt;- lapply( 1997:2009, yr_pop_vec ) proj_pop &lt;- function( i ) { sum( all_mat[,,i] %*% year_pop[[i]] ) } projected_pop_ns &lt;- sapply( 1:13, proj_pop ) pop_counts_update &lt;- pop_counts %&gt;% mutate( proj_n_t1 = projected_pop_ns ) %&gt;% mutate( proj_pgr = proj_n_t1/n_t0 ) # png( &#39;results/Bou_gra_yr/obs_proj_lambdas.png&#39;, width = 6, height = 4, units = &quot;in&quot;, res = 150 ) ggplot( pop_counts_update ) + geom_point( aes( x = lambda, y = obs_pgr ), color = &#39;brown&#39; ) + geom_point( aes( x = proj_pgr, y = obs_pgr ), color = &#39;red&#39; ) + geom_abline( aes(intercept = 0, slope = 1) ) + labs( x = &quot;Modeled lambda&quot;, y = &quot;Observed population growth rate&quot; ) # dev.off( ) Year nt0 nt1 Observedpopulation growth Yearly asymptoticlambda Projectednt1 Projectedpopulation growth 1998 42 58 1.381 1.277 69 1.646 1999 58 159 2.741 2.291 151 2.608 2000 49 84 1.714 0.947 120 2.446 2001 84 74 0.881 1.076 97 1.15 2002 174 934 5.368 3.678 653 3.75 2003 934 286 0.306 0.907 363 0.388 2004 286 283 0.99 0.896 214 0.748 2005 283 173 0.611 0.974 216 0.763 2006 173 233 1.347 1.257 213 1.23 2007 233 314 1.348 1.165 276 1.186 2008 314 342 1.089 1.027 325 1.035 2009 342 245 0.716 0.972 267 0.78 2010 245 220 0.898 0.967 202 0.826 Once we are satisfied with our IPM, we can begin constructing it with ipmr. 6.0.7 10. Building the year-specific IPMs with ipmr As in the previous sections, we will rely heavily on the code we already wrote in Chapter 4. However, we will now implement the _yr suffix and utilize the parameter set functionality in ipmr. The first step is to stick all of our varying and constant parameters into a single list to give to ipmr. all_pars &lt;- c( pars_cons_wide, pars_var_wide ) # write.csv( all_pars, &quot;data/Bou_gra_yr/all_pars.csv&quot;, row.names = F ) Now let’s build the proto-IPM. Some comments have been included in the following code to explain the new pieces, but refer back to Chapter 4.4.10 for more details about constructing the IPM in ipmr. I’m jumping right in to the full proto-IPM with both kernels. Note the use of the _yr suffix on all terms AND expressions which vary across the years, every time they are mentioned!!! library( ipmr ) proto_ipm_yr &lt;- init_ipm( sim_gen = &quot;simple&quot;, di_dd = &quot;di&quot;, det_stoch = &quot;det&quot; ) %&gt;% define_kernel( name = &quot;P_yr&quot;, family = &quot;CC&quot;, formula = s_yr * g_yr, s_yr = plogis( surv_b0_yr + surv_b1_yr * size_1), g_yr = dnorm( size_2, mu_g_yr, grow_sig ), mu_g_yr = grow_b0_yr + grow_b1_yr * size_1 + grow_b2_yr * size_1^2, grow_sig = sqrt( a * exp( b * size_1 ) ), data_list = all_pars, states = list( c( &#39;size&#39; ) ), # these next two lines are new # the first tells ipmr that we are using parameter sets uses_par_sets = TRUE, # the second defines the values the yr suffix can assume par_set_indices = list( yr = 1997:2009 ), evict_cor = TRUE, evict_fun = truncated_distributions( fun = &#39;norm&#39;, target = &#39;g_yr&#39; ) ) %&gt;% define_kernel( name = &#39;F_yr&#39;, family = &#39;CC&#39;, formula = fecu_b0_yr * r_d, r_d = dnorm( size_2, recr_sz, recr_sd ), data_list = all_pars, states = list( c( &#39;size&#39; ) ), uses_par_sets = TRUE, par_set_indices = list( yr = 1997:2009 ), evict_cor = TRUE, evict_fun = truncated_distributions( &quot;norm&quot;, &quot;r_d&quot; ) ) %&gt;% define_impl( make_impl_args_list( kernel_names = c( &quot;P_yr&quot;, &quot;F_yr&quot; ), int_rule = rep( &quot;midpoint&quot;, 2 ), state_start = rep( &quot;size&quot;, 2 ), state_end = rep( &quot;size&quot;, 2 ) ) ) %&gt;% define_domains( size = c(all_pars$L, all_pars$U, all_pars$mat_siz ) ) %&gt;% # We also append the suffix in define_pop_state(). This will create a deterministic # simulation for every &quot;year&quot; define_pop_state( n_size_yr = rep( 1 / 200, 200 ) ) ipmr_yr &lt;- make_ipm( proto_ipm = proto_ipm_yr, iterations = 200 ) lam_mean_ipmr &lt;- lambda( ipmr_yr ) Year nt0 nt1 Observedpopulation growth Yearly asymptoticlambda Projectednt1 Projectedpopulation growth Lambda from ipmr 1998 42 58 1.381 1.277 69 1.646 1.286 1999 58 159 2.741 2.291 151 2.608 2.295 2000 49 84 1.714 0.947 120 2.446 0.955 2001 84 74 0.881 1.076 97 1.15 1.085 2002 174 934 5.368 3.678 653 3.75 3.68 2003 934 286 0.306 0.907 363 0.388 0.747 2004 286 283 0.99 0.896 214 0.748 0.803 2005 283 173 0.611 0.974 216 0.763 0.941 2006 173 233 1.347 1.257 213 1.23 1.256 2007 233 314 1.348 1.165 276 1.186 1.167 2008 314 342 1.089 1.027 325 1.035 1.034 2009 342 245 0.716 0.972 267 0.78 0.909 2010 245 220 0.898 0.967 202 0.826 0.893 These lambda values seem pretty good! Let’s export them as a dataframe. lam_out &lt;- data.frame( coefficient = names( lam_mean_ipmr ), value = lam_mean_ipmr ) rownames( lam_out ) &lt;- 1:13 lam_out_wide &lt;- as.list( pivot_wider( lam_out, names_from = &quot;coefficient&quot;, values_from = &quot;value&quot; ) ) # write.csv( lam_out_wide, &quot;data/Bou_gra_yr/lambdas_yr.csv&quot;, row.names = F ) Now, we can go ahead and begin filling in the database template. 6.0.8 11. Populating the PADRINO database template Since we are implementing the _yr suffix syntax to build all of our IPMs with parameter sets, we can actually store all 13 of these IPMs under a single IPM ID! For the sake of this example, we will use “bg00yr” as our IPM ID. In many tables, the only difference between what we have already entered in Chapter 4 and what we will enter now is the addition of the _yr suffix when appropriate, following the changes we made to the IPM in the last step. I will point out when tables change, as necessary. The biggest changes will come in the ParameterValues table, because we will need to load in many more values than previously. Let’s start by loading in the Excel file of the empty template. Download the template from here and store it locally. Paste in the path to this script: library( readxl ) YOUR_PATH &lt;- &quot;this is where you should paste the path to the folder where you&#39;ve stored your file&quot; sheet_names &lt;- excel_sheets( paste( YOUR_PATH, &quot;/pdb_template.xlsx&quot;, sep = &quot;&quot; ) ) pdb &lt;- lapply( sheet_names, function( x ) { as.data.frame( read_excel( paste( YOUR_PATH, &quot;/pdb_template.xlsx&quot;, sep = &quot;&quot; ), sheet = x ) ) } ) names( pdb ) &lt;- sheet_names Next, we can populate the Metadata table. Note that we are changing the IPM ID and the kernel names. pdb$Metadata[1,] &lt;- c( &quot;bg00yr&quot;, # Taxonomic information &quot;Bouteloua gracilis&quot;, &quot;Bouteloua gracilis&quot;, &quot;Bouteloua&quot;, &quot;Poaceae&quot;, &quot;Poales&quot;, &quot;Liliopsida&quot;, &quot;Magnoliophyta&quot;, &quot;Plantae&quot;, &quot;Herbaceous&quot;, &quot;Monocot&quot;, &quot;angio&quot;, # Publication information &quot;Chu; Norman; Flynn; Kaplan; Lauenroth; Adler&quot;, &quot;Ecology&quot;, &quot;2013&quot;, &quot;10.1890/13-0121.1&quot;, &quot;Adler&quot;, &quot;peter.adler@usu.edu (2023)&quot;, NA, &quot;Chu, C., Norman, J., Flynn, R., Kaplan, N., Lauenroth, W.K. and Adler, P.B. (2013), Cover, density, and demographics of shortgrass steppe plants mapped 1997–2010 in permanent grazed and ungrazed quadrats. Ecology, 94: 1435-1435. https://doi.org/10.1890/13-0121.1&quot;, &quot;https://doi.org/10.6084/m9.figshare.c.3305970.v1&quot;, # Data collection information 14, 1997, NA, 2010, NA, 1, &quot;Shortgrass Steppe LTER&quot;, &quot;6&quot;, &quot;40.84519843&quot;, &quot;-104.7107395&quot;, &quot;1652.2&quot;, &quot;USA&quot;, &quot;n_america&quot;, &quot;TGS&quot;, # Model information &quot;A&quot;, TRUE, &quot;truncated_distributions&quot;, &quot;P_yr; F_yr&quot;, NA, FALSE, FALSE, FALSE, FALSE, &quot;&quot;, &quot;&quot;, &quot;&quot; ) pdb$Metadata$eviction_used &lt;- as.logical(pdb$Metadata$eviction_used) pdb$Metadata$duration &lt;- as.numeric(pdb$Metadata$duration) pdb$Metadata$periodicity &lt;- as.numeric(pdb$Metadata$periodicity) In the StateVariables, ContinuousDomains, IntegrationRules, and StateVectors tables, we also only change the IPM ID and add the suffix to the kernel names: pdb$StateVariables[1,] &lt;- c( &quot;bg00yr&quot;, &quot;size&quot;, FALSE) pdb$StateVariables$discrete &lt;- as.logical( pdb$StateVariables$discrete ) pdb$ContinuousDomains[1,] &lt;- c( &quot;bg00yr&quot;, &quot;size&quot;, &quot;&quot;, all_pars$L, all_pars$U, &quot;P_yr; F_yr&quot;, &quot;&quot; ) pdb$ContinuousDomains$lower &lt;- as.numeric( pdb$ContinuousDomains$lower ) pdb$ContinuousDomains$upper &lt;- as.numeric( pdb$ContinuousDomains$upper ) pdb$IntegrationRules[1,] &lt;- c( &quot;bg00yr&quot;, &quot;size&quot;, &quot;&quot;, all_pars$mat_siz, &quot;midpoint&quot;, &quot;P_yr; F_yr&quot; ) pdb$IntegrationRules$n_meshpoints &lt;- as.numeric( pdb$IntegrationRules$n_meshpoints ) pdb$StateVectors[1,] &lt;- c( &quot;bg00yr&quot;, &quot;n_size&quot;, all_pars$mat_siz, &quot;&quot; ) pdb$StateVectors$n_bins &lt;- as.numeric( pdb$StateVectors$n_bins ) At the IpmKernels table, we again change the IPM ID and kernel names, but we also add the _yr suffix to the vital rate expression names. pdb$IpmKernels[1,] &lt;- c( &quot;bg00yr&quot;, &quot;P_yr&quot;, &quot;P_yr = s_yr * g_yr * d_size&quot;, &quot;CC&quot;, &quot;size&quot;, &quot;size&quot; ) pdb$IpmKernels[2,] &lt;- c( &quot;bg00yr&quot;, &quot;F_yr&quot;, &quot;F_yr = fy_yr * d_size&quot;, &quot;CC&quot;, &quot;size&quot;, &quot;size&quot; ) We follow through with this in the VitalRateExpr sheet, making sure to also update the growth vital rate expression with the new model! pdb$VitalRateExpr[1,] &lt;- c( &quot;bg00yr&quot;, &quot;Survival&quot;, &quot;s_yr = 1 / ( 1 + exp( -( surv_b0_yr + surv_b1_yr * size_1 ) ) )&quot;, &quot;Evaluated&quot;, &quot;P_yr&quot; ) pdb$VitalRateExpr[2,] &lt;- c( &quot;bg00yr&quot;, &quot;Growth&quot;, &quot;mu_g_yr = grow_b0_yr + grow_b1_yr * size_1 + grow_b2_yr * size_1^2&quot;, &quot;Evaluated&quot;, &quot;P_yr&quot; ) pdb$VitalRateExpr[3,] &lt;- c( &quot;bg00yr&quot;, &quot;Growth&quot;, &quot;g_yr = Norm( mu_g_yr, sd_g )&quot;, &quot;Substituted&quot;, &quot;P_yr&quot; ) pdb$VitalRateExpr[4,] &lt;- c( &quot;bg00yr&quot;, &quot;Growth&quot;, &quot;sd_g = sqrt( a * exp( b * size_1 ) )&quot;, &quot;Evaluated&quot;, &quot;P_yr&quot; ) pdb$VitalRateExpr[5,] &lt;- c( &quot;bg00yr&quot;, &quot;Fecundity&quot;, &quot;fy_yr = fecu_b0_yr * r_d&quot;, &quot;Evaluated&quot;, &quot;F_yr&quot; ) pdb$VitalRateExpr[6,] &lt;- c( &quot;bg00yr&quot;, &quot;Fecundity&quot;, &quot;r_d = Norm( recr_sz, recr_sd )&quot;, &quot;Substituted&quot;, &quot;F_yr&quot; ) From this point forward, we will be making bigger changes. In the ParameterValues sheet, we need to enter the parameter values which will remain constant throughout all years (a, b, recr_sz, recr_sd), but we will also need to enter all values of the parameters which vary across the years. To accomplish this, I will use a for loop, even though this is a less efficient approach. The for loop will populate the first 78 rows of the sheet with the varying parameters, and then I will manually enter the constant parameters afterwards. for( i in 1:( length( pars_var_wide ) ) ) { pdb$ParameterValues[i,1] &lt;- &quot;bg00yr&quot; pdb$ParameterValues[i,3] &lt;- &quot;size&quot; pdb$ParameterValues[i,4] &lt;- names( pars_var_wide )[i] pdb$ParameterValues[i,5] &lt;- as.numeric( pars_var_wide[i] ) if( grepl( &quot;surv&quot;, names( pars_var_wide )[i] ) ){ pdb$ParameterValues[i,2] &lt;- &quot;Survival&quot; } else { if( grepl( &quot;grow&quot;, names( pars_var_wide )[i] ) ){ pdb$ParameterValues[i,2] &lt;- &quot;Growth&quot; } else { pdb$ParameterValues[i,2] &lt;- &quot;Fecundity&quot; } } } pdb$ParameterValues[79,] &lt;- c( &quot;bg00yr&quot;, &quot;Growth&quot;, &quot;size&quot;, &quot;a&quot;, all_pars$a ) pdb$ParameterValues[80,] &lt;- c( &quot;bg00yr&quot;, &quot;Growth&quot;, &quot;size&quot;, &quot;b&quot;, all_pars$b ) pdb$ParameterValues[81,] &lt;- c( &quot;bg00yr&quot;, &quot;Fecundity&quot;, &quot;size&quot;, &quot;recr_sz&quot;, all_pars$recr_sz ) pdb$ParameterValues[82,] &lt;- c( &quot;bg00yr&quot;, &quot;Fecundity&quot;, &quot;size&quot;, &quot;recr_sd&quot;, all_pars$recr_sd ) pdb$ParameterValues$parameter_value &lt;- as.numeric( pdb$ParameterValues$parameter_value ) As in Chapter 4, we skip the EnvironmentalVariables table, but we will need to populate the ParSetIndices this time. This table tells ipmr the values which our _yr suffix can take. pdb$ParSetIndices[1,] &lt;- c( &quot;bg00yr&quot;, &quot;year&quot;, &quot;yr&quot;, &quot;1997:2009&quot;, &quot;P_yr; F_yr&quot;, &quot;&quot; ) Again, we skip the UncertaintyTable and move onto the last table, TestTargets. Like our ParameterValues table, we also need to enter year-specific information, this time those year-specific lambdas. pdb$TestTargets[1:13,1] &lt;- &quot;bg00yr&quot; pdb$TestTargets[1:13,2] &lt;- names(lam_mean_ipmr) pdb$TestTargets[1:13,3] &lt;- as.numeric(lam_mean_ipmr) pdb$TestTargets[1:13,4] &lt;- 3 pdb$TestTargets$target_value &lt;- as.numeric( pdb$TestTargets$target_value ) pdb$TestTargets$precision &lt;- as.numeric( pdb$TestTargets$precision ) Once all of the necessary sheets have been filled, export the pdb file to Excel, storing it in the project home folder. library( writexl ) write_xlsx( pdb, &quot;Bou_gra_yr_pdb.xlsx&quot; ) Now, we must test the pdb file to make sure ipmr can recreate the models from the Excel tables. library( pdbDigitUtils ) pdb_test &lt;- read_pdb( &quot;Bou_gra_yr_pdb.xlsx&quot; ) pdb_test_proto &lt;- pdb_make_proto_ipm( pdb_test, det_stoch = &quot;det&quot; ) print( pdb_test_proto$bg00yr ) bg_ipm_pdb &lt;- make_ipm( pdb_test_proto$bg00yr ) bg_ipm_pdb lambda( bg_ipm_pdb ) test_model( pdb_test, id = &quot;bg00yr&quot; ) Following all of these steps exactly, I did not get any errors and the model passed the test! 6.0.9 12. Finishing up the digitization project If we’ve been following along with all of the steps, all of the files should be organized correctly! We just need to finalize the notes and wrapper files. The notes file, “Bou_gra_yr_notes.txt,” should contain the following information: Thirteen year-specific simple, deterministic, density-independent IPMs were constructed from these data. All data were aggregated by treatment to fit the vital rate models. Raw data were loaded from the “Bou_gra” mean model digitization project; no more extra filtering steps took place. The year-specific recruitment rates were calculated as the mean per-capita rates of recruitment across all plots, based on the number of adults observed at time t0 of a given year and the number of recruits observed at time t1 (the following year). The parameters of the growth variance model, a and b, as well as the parameters defining the probability density distribution describing recruit size, recruit size mean and standard deviation, were held constant across all years, while all other vital rate parameters varied across the years. As before, let’s take a look at the organization of the project folder before writing the wrapper file:  Adler_Colorado ↳  Adler_Colorado.Rproj  data ↳  Bou_gra  Bou_gra_yr ↳  surv_pars.csv  grow_pars.csv  recr_pars.csv  pars_cons.csv  pars_var.csv  all_pars.csv  lambdas_yr.csv  R ↳  Bou_gra  Bou_gra_yr ↳ Ⓡ 01_Plotting.R Ⓡ 02_VitalRates.R Ⓡ 03_IPMScratch.R Ⓡ 04_ipmr.R Ⓡ 05_PADRINO.R  results ↳  Bou_gra  Bou_gra_yr ↳  histograms.png  survival_binned_yr.png  growth_yr.png  recruit_yr.png  recr_histograms.png  survival_pred.png  grow_pred.png  grow_pred_2.png  recruit_pred.png  obs_proj_lambdas.png  Chu_2013_fulltext.pdf  Bou_gra_pdb.xlsx  Bou_gra_yr_pdb.xlsx  Bou_gra_notes.txt  Bou_gra_yr_notes.txt Ⓡ Bou_gra_wrapper.R Ⓡ Bou_gra_yr_wrapper.R And finally, let’s write the wrapper: # Chu et al. 2013: Bouteloua gracilis year-specific IPMs for PADRINO # Aspen Workman, summer 2024 # Setup library( bblme ) # ver 1.0.25 library( ggplot2 ) # ver 3.4.4 library( htmlTable ) # ver 2.4.2 library( ipmr ) # ver 0.0.7 library( lme4 ) # ver 1.1-33 library( patchwork ) # ver 1.1.2 library( pdbDigitUtils ) # ver 0.0.0.90 library( readxl ) # ver 1.4.2 library( tidyverse ) # ver 2.0.0 library( writexl ) # ver 1.4.2 base_dir &lt;- dirname( rstudioapi::getActiveDocumentContext()$path ) R_dir &lt;- paste( base_dir, &quot;/R/Bou_gra_yr/&quot;, sep = &quot;&quot; ) setwd( base_dir ) # Overview --------------------------------------------------------------------- # This R project concerns the construction of IPMs based on chart quadrat data # from Chu et al., 2013. This file focuses on the dataset of Bouteloua # gracilis, which was converted from chart quadrat data to demographic data by # A. Workman and A. Compagnoni using plantTracker. The dataset consists of 13 # transitions from 24 plots in Colorado, USA. Thirteen year-specific, simple, # deterministic, density-independent IPMs were constructed from these data, # representing the year-specific models. These IPMs were formatted for # inclusion in the PADRINO database. # Citation of data source: # Chu, C., Norman, J., Flynn, R., Kaplan, N., Lauenroth, W.K. and Adler, P.B. # (2013), Cover, density, and demographics of shortgrass steppe plants mapped # 1997–2010 in permanent grazed and ungrazed quadrats. Ecology, 94: 1435-1435. # https://doi.org/10.1890/13-0121.1 # Project organization --------------------------------------------------------- # Contained within the R project are all of the files needed to construct and # test multiple IPMs. The files which refer to the year-specific models for # Bouteloua gracilis are indicated by the abbreviation &quot;Bou_gra_yr.&quot; The home # folder of the project contains the finalized PADRINO database Excel file # (&quot;Bou_gra_yr_pdb.xlsx&quot;), a text file describing decisions that were made # about the data during the modeling process (&quot;Bou_gra_yr_notes.txt&quot;) and this # wrapper file. # Input data are stored in the &#39;data/Bou_gra&#39; subfolder. The raw data, # &quot;Bou_gra.csv&quot;, is the output from plantTracker filtered for only occurrences # of Bouteloua gracilis. The data files &quot;Bou_gra_clean.csv&quot;, and the # dataframes ending in &quot;_mod.csv&quot; were used to fit the vital rate models for # the year-specific IPMs. The steps to generate these input data files are # contained within the R scripts found in the &#39;R/Bou_gra&#39; subfolder. All data # files which are outputted during the construction of the year-specific IPMs # are stored in the &#39;data/Bou_gra_yr&#39; subfolder, and the steps to generate # these files are contained within the R scripts found in the &#39;R/Bou_gra_yr&#39; # subfolder. # The R scripts are stored in the &#39;R/Bou_gra_yr&#39; subfolder. The number prefix of # the file name indicates the order in which the scripts should be executed. # Plots which were created throughout the course of this project are stored in # the &#39;results/Bou_gra_yr&#39; subfolder. Plots with the &quot;_pred&quot; suffix in the # file name indicate visualizations of the vital rate models, while all other # plots are visualizations of the raw data. # Workflow --------------------------------------------------------------------- # Visualize the raw data source( paste0( R_dir, &quot;01_Plotting.R&quot; ) ) # Fit vital rate models source( paste0( R_dir, &quot;02_VitalRates.R&quot; ) ) # Construct IPMs from scratch source( paste0( R_dir, &quot;03_IPMScratch.R&quot; ) ) # Construct IPMs using ipmr syntax source( paste0( R_dir, &quot;04_ipmr.R&quot; ) ) # Populate the PADRINO database template source( paste0( R_dir, &quot;05_PADRINO.R&quot; ) ) As before, now that we’ve finished the wrapper file and all of the files are organized, we should make sure the entire project is uploaded or otherwise synced to whatever cloud service we’ve decided to use. Since we’ve already updated the Incoming literature Google Sheet, all that is left is to celebrate! "],["useful-links.html", "A Useful links", " A Useful links Here is a collection of (nearly) all of the links which are referenced throughout this guide, nicely organized into a single location for easy reference! See Chapter 4 for a collection of links related to the Adler datasets. PADRINO Digitization Guide: Guide for reformatting IPMs for inclusion in the PADRINO database. Tidyverse style guide: Guidelines for code formatting on which Chapter 1 is based, extended version. “Incoming literature” Google Sheet: Check out the publications which have already been added to PADRINO, find publications which still need to be digitized, and add publications you’ve found to the queue. Sam’s guide to Model Expressions: How to write vital rate models for IPMs. Contacting authors for missing information: Email template to contact authors for information you need to digitize their IPM. What biome code do I use?: Input the coordinates of the sampled site here to obtain the three-letter biome code to put into the Metadata table. The ipmr site: Information about the ipmr package that PADRINO is built on. “plantTracker: An R package to translate maps of plant occurrence into demographic data”: Stears et al., 2022; publication introducing plantTracker. Guide to plantTracker: Get started using plantTracker. pdb Template: Blank PADRINO database template Excel file to download and populate. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
